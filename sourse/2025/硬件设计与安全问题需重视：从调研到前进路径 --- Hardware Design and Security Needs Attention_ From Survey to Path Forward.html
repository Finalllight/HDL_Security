<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Hardware Design and Security Needs Attention: From Survey to Path Forward</title>
<!--Generated on Mon Jun 16 22:22:58 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Attention,  Transformer,  Large Language Models (LLMs),  Electronic Design Automation (EDA),  Hardware Design,  Hardware Security.
" lang="en" name="keywords"/>
<base href="/html/2504.08854v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S1" title="In Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2" title="In Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background and Motivation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.SS1" title="In II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">AI and Machine Learning</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.SS1.SSS1" title="In II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>Attention Mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.SS1.SSS2" title="In II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>2 </span>Graph Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.SS1.SSS3" title="In II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>3 </span>Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.SS2" title="In II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Hardware Design and Challenges </span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.SS3" title="In II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Hardware Security and Challenges</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3" title="In Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Attention-Based Hardware design</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS1" title="In III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">LLM-based Digital Hardware Design</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS1.SSS1" title="In III-A LLM-based Digital Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span>Design And Specification Assistance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS1.SSS2" title="In III-A LLM-based Digital Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span>Optimization And Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS1.SSS3" title="In III-A LLM-based Digital Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span>Verification And Validation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS1.SSS4" title="In III-A LLM-based Digital Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>4 </span>Debugging And Maintenance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS2" title="In III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">LLM-based Analog Hardware Design</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.SS3" title="In III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Other Attention-based Hardware Design</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4" title="In Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Attention-Based hardware security</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS1" title="In IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">LLM-based Hardware Security</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS1.SSS1" title="In IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Vulnerability Detection And Threat Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS1.SSS2" title="In IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>SoC Security</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS1.SSS3" title="In IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>Trojan Insertion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS1.SSS4" title="In IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>4 </span>Logic Obfuscation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS1.SSS5" title="In IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>5 </span>Side-Channel Attack</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.SS2" title="In IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Other Attention-based Hardware Security </span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S5" title="In Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Future Direction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S6" title="In Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hardware Design and Security Needs Attention: 
<br class="ltx_break"/>From Survey to Path Forward</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sujan Ghimire<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Muhtasim Alam Chowdhury<sup class="ltx_sup" id="id2.2.id2">2</sup>,
Banafsheh Saber Latibari<sup class="ltx_sup" id="id3.3.id3">2</sup>,
Muntasir Mamun<sup class="ltx_sup" id="id4.4.id4">1</sup>,
<br class="ltx_break"/>Jaeden Wolf Carpenter<sup class="ltx_sup" id="id5.5.id5">2</sup>,
Benjamin Tan<sup class="ltx_sup" id="id6.6.id6">3</sup>,
Hammond Pearce<sup class="ltx_sup" id="id7.7.id7">4</sup>,
Krishnendu Chakrabarty<sup class="ltx_sup" id="id8.8.id8">5</sup>, 
<br class="ltx_break"/>Pratik Satam<sup class="ltx_sup" id="id9.9.id9">1</sup>,
and Soheil Salehi<sup class="ltx_sup" id="id10.10.id10">2</sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id11.11.id1">1</sup>Systems and Industrial Engineering Department, University of Arizona, Tucson, AZ, USA (e-mail: sghimire@arizona.edu, muntasir@arizona.edu, pratiksatam@arizona.edu).<sup class="ltx_sup" id="id12.12.id1">2</sup>Electrical and Computer Engineering Department, University of Arizona, Tucson, AZ, USA (e-mail: mmc7@arizona.edu, banafsheh@arizona.edu, carpenterjaeden@arizona.edu, ssalehi@arizona.edu).<sup class="ltx_sup" id="id13.13.id1">3</sup>Department of Electrical and Software Engineering, University of Calgary, Calgary, Alberta, CA (e-mail: benjamin.tan1@ucalgary.ca).<sup class="ltx_sup" id="id14.14.id1">4</sup>School of Computer Science and Engineering, University of New South Wales, Sydney, AU (e-mail: hammond.pearce@unsw.edu.au).<sup class="ltx_sup" id="id15.15.id1">5</sup>School of Electrical Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA (e-mail: Krishnendu.Chakrabarty@asu.edu).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.id1">Recent advances in attention-based artificial intelligence (AI) models have unlocked vast potential to automate digital hardware design while enhancing and strengthening security measures against various threats. This rapidly emerging field leverages Large Language Models (LLMs) to generate HDL code, identify vulnerabilities, and sometimes mitigate them. The state of the art in this design automation space utilizes optimized LLMs with HDL datasets, creating automated systems for register transfer level(RTL) generation, verification, and debugging and establishing LLM-driven design environments for streamlined logic designs. Additionally, attention-based models like graph attention have shown promise in chip design applications, including floorplanning. This survey investigates the integration of these models into hardware-related domains, emphasizing logic design and hardware security, with or without the use of IP libraries. This study explores the commercial and academic landscape, highlighting technical hurdles and future prospects for automating hardware design and security.
Moreover, it provides new insights into the study of LLM-driven design systems, advances in hardware security mechanisms, and the impact of influential works on industry practices. Through the examination of 30 representative approaches and illustrative case studies, this paper underscores the transformative potential of attention-based models in revolutionizing hardware design while addressing the challenges that lie ahead in this interdisciplinary domain.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Attention, Transformer, Large Language Models (LLMs), Electronic Design Automation (EDA), Hardware Design, Hardware Security.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Hardware design is a complex process that begins with specific requirements outlined in natural language. It involves hardware engineers translating these requirements into Hardware Description Languages (HDLs) before verifying them extensively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib2" title="">2</a>]</cite>. During this process, hardware engineers must ensure that design constraints (such as performance, power, and area) are met, which can be repetitive and prone to errors and inefficiencies.
Automating these processes with machine learning (ML) models, such as recent deep neural networks featuring the <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">attention</span> mechanism¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib3" title="">3</a>]</cite>, can significantly reduce the likelihood of human error and accelerate the design cycle by streamlining tasks and improving design efficiency. Initially, the attention-based model was introduced as an alternative to recurrent neural networks (RNNs) for natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib3" title="">3</a>]</cite>.
The transformer model, which uses self-attention to focus on different parts of input data to make predictions, has led to pioneering outcomes in many domains like speech recognition, computer vision, sentiment analysis, named entity recognition, and time series analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib6" title="">6</a>]</cite>. LLMs such as the Generative Pre-trained Transformer (GPT) series have demonstrated remarkable capabilities in generating human-like text, understanding context, and performing complex language-related tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="313" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Timeline of AI advancements and their integration into hardware design and security from 2017 to 2024. </figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">1</span></a> shows the evolution of key advancements in AI and hardware design, from the introduction of google transformer architecture (2017) to the rise of LLMs like GPT-3 (2020), their application in chip design (2023), the first hardware tapeout of hardware designed with LLMs (2023), and advancements in security mechanisms using LLMs (2024).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the realm of hardware design and security, the application of attention-based models presents a transformative opportunity still in its early stages of exploration. Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S1.F2" title="Figure 2 ‚Ä£ I Introduction ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">2</span></a> shows the projection for the number of publications through 2025.
The numbers presented in this figure are derived from an extensive survey of 117 references included in this paper. These references encompass studies on applying attention-based models in hardware design and security. The trajectory in the figure was determined by categorizing and analyzing the publication trends of the surveyed resources, reflecting the growing interest and progress in this interdisciplinary domain. Specifically, the prediction for 2025, where the number of articles is expected to increase to 345, was extrapolated based on the observed growth pattern, which increased fourfold from 21 articles in 2023 to 86 in 2024, indicating a compounding trend.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="424" id="S1.F2.g1" src="x2.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Projected growth in research publications on LLM-based hardware design and hardware security, highlighting a nearly 4x increase from 2023 to 2024, with an estimated surge in 2025 following the same growth pattern.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recent work has explored the potential of LLMs for automated HDL generations from instructions in Natural Language (known as ‚Äòprompts‚Äô)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib8" title="">8</a>]</cite>. Although this showed potential for speeding up the design process (including successful tape-out) by prompting LLMs, significant human intervention was still required, as pre-trained attention-based models could not handle all the tasks necessary for a complete tape-out design without errors.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Overcoming this performance gap between LLMs and humans is not trivial.
One of the significant challenges here is the relative scarcity of public HDL code bases usable for LLM fine-tuning. Notable initiatives include optimizing previously pretrained LLMs using Verilog datasets, creating automated frameworks such as Autochip<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib9" title="">9</a>]</cite> for bug correction in LLM-generated HDL code, and creating LLM-based design environments. These initiatives represent initial steps in adapting LLMs to the hardware engineering landscape, focusing on logic design optimization through natural language interaction¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib9" title="">9</a>]</cite>. Other research using attention-based models for the rest of the chip design stages has also faced the challenge of limited data.</p>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="442" id="S1.F3.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Funding distribution across various research areas and organizations, illustrating the contributions of multiple funding sources. The chart highlights the diversity and extent of support provided to hardware design, verification, security, and optimization research. </figcaption>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address the aforementioned limitations and make the semiconductor industry more efficient, significant interest and support have been directed toward this research area. Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S1.F3" title="Figure 3 ‚Ä£ I Introduction ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">3</span></a> shows a snapshot of some of the funding resources supporting this research. To compile this data, we targeted the references that are cited in this paper to identify the key funding agencies supporting research in attention-based hardware design and security. The data reflects a total of 38 funding instances across various research areas, with notable contributions from leading organizations such as NSF, NSFC, SRC, and DARPA. Furthermore, the funding distribution underscores the growing research interest in attention-based hardware design and security, reflecting its increasing significance in shaping future advancements in the field.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In recent work, Bei Yu summarized the
application of ML techniques in Electronic Design Automation (EDA)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib10" title="">10</a>]</cite>.
He highlighted their impact on various stages of chip design and proposed an adaptive integrated ML paradigm to address challenges such as netlist representation, timing modeling, and multimodal fusion. LLM4EDA provided a systematic study on the application of LLMs in EDA, exploring their potential to automate various stages of chip design and improve power, performance, and area (PPA) metrics while highlighting future research directions and maintaining an open-source repository of relevant papers and datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib11" title="">11</a>]</cite>.
Tsung-Yi Ho et al. explored the development and application of AI-native large circuit models (LCMs) in EDA, highlighting their potential to revolutionize the field through comprehensive analysis and creation capabilities while addressing challenges such as data scarcity and scalability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib12" title="">12</a>]</cite>.
Sharma et al. conducted a thorough analysis of the total cost of ownership (TCO) and performance between ChipNeMo, a domain-adaptive LLM, and leading general-purpose LLMs (Claude 3 Opus and ChatGPT-4 Turbo), demonstrating significant cost savings and performance benefits for ChipNeMo in chip design coding tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib13" title="">13</a>]</cite>.
Nan Wu et al. provided a comprehensive overview of ML methodologies applied to hardware design and verification, identifying challenges and proposing solutions within EDA, highlighting the potential and limitations of ML in enhancing design efficiency and accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib14" title="">14</a>]</cite>.
Yang et al. presented an in-depth review of LLMs, focusing on their applications, capabilities, challenges, and prospects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib15" title="">15</a>]</cite>. It explores evaluation metrics like BLEU scores and human-aligned evaluations, emphasizes the importance of prompt engineering for LLM efficacy, compares fine-tuning with prompt tuning, discusses scalability and generalization capabilities, and addresses ethical concerns such as bias and misinformation.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">An emerging challenge in hardware design is the need to incorporate security.
Akyash et al. explored LLMs in hardware security, detailing their use for RTL vulnerability detection and mitigation, evaluation frameworks, comparative analyses, case studies, and future needs for specialized architectures, improved datasets, and EDA tool integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib16" title="">16</a>]</cite>.
Similarly, Pearce et al. argue for the potential of LLMs in this space, highlighting successes already obtained by LLMs working in limited hardware security tasks and scenarios¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">This paper offers a complementary extension to prior surveys. In considering all types of models based on attention mechanisms, we do not limit our survey purely to LLMs, and we survey the use of these models in all design stages, from RTL generation and verification to layout design and routing.
Further, we provide insights into both hardware <span class="ltx_text ltx_font_italic" id="S1.p9.1.1">design</span> and hardware <span class="ltx_text ltx_font_italic" id="S1.p9.1.2">security</span>, such as Trojan insertion and logic obfuscation.</p>
</div>
<figure class="ltx_figure" id="S1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="716" id="S1.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overview of the survey in this manuscript.</figcaption>
</figure>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S1.F4" title="Figure 4 ‚Ä£ I Introduction ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">4</span></a> shows the organization of this paper.
Section <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2" title="II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">II</span></a> provides background on ML models used in the literature. Moreover, the limitations and challenges that make using these attention-based models necessary in hardware design and security are discussed. Sections <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3" title="III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">III</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4" title="IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">IV</span></a> summarize the attention-based hardware design and security, respectively. We talk about the future directions of the research in Section <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S5" title="V Future Direction ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">V</span></a>, and Section <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S6" title="VI Conclusion ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes the paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background and Motivation</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we provide a comprehensive background on the evolution of AI and ML, with a particular focus on the transformative advancements in neural network architectures. We begin by introducing the foundational concepts of AI and ML, followed by a detailed discussion on graph attention mechanisms and the revolutionary transformer model, highlighting their development and impact over the years. Furthermore, we offer an in-depth overview of these concepts, emphasizing their significance in modern computational paradigms. Subsequently, we transition to the domain of hardware design, exploring the challenges associated with it. Finally, we address the critical aspect of hardware security, examining the unique vulnerabilities and challenges in hardware design.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">AI and Machine Learning</span>
</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.4.1.1">II-A</span>1 </span>Attention Mechanism</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.5">The attention mechanism is a neural network structure engineered to concentrate on the most relevant aspects of the input data during prediction.
Bahdanau et al. (2015) initially developed it in neural machine translation to overcome the shortcomings of conventional models such as Recurrent Neural Networks (RNNs), which struggle to capture long-range relationships and manage variable-length sequences¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib18" title="">18</a>]</cite>.
The core principle of attention is computing a weighted sum of input items, where the weights reflect the importance of each input component.
This enables the model to focus on critical elements of the input, enhancing its ability to comprehend complex data. Various types of attention processes are available, such as global attention, which considers the whole input sequence, and local attention, which concentrates on certain portions of the sequence. Self-attention enables each element in the input to focus on all other elements, constituting a fundamental aspect of transformer models.
Furthermore, multi-head attention utilizes many attention processes concurrently, allowing the model to discern several facets of the input.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F5" title="Figure 5 ‚Ä£ II-A1 Attention Mechanism ‚Ä£ II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">5</span></a>-(a) illustrates the attention mechanism focusing on the scaled dot-product attention. The mechanism begins with the inputs, queries (<math alttext="Q" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.1.m1.1"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><mi id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><ci id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.1.m1.1d">italic_Q</annotation></semantics></math>), keys (<math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.2.m2.1"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><mi id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><ci id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.2.m2.1d">italic_K</annotation></semantics></math>), and values (<math alttext="V" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.3.m3.1"><semantics id="S2.SS1.SSS1.p1.3.m3.1a"><mi id="S2.SS1.SSS1.p1.3.m3.1.1" xref="S2.SS1.SSS1.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.3.m3.1b"><ci id="S2.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.3.m3.1d">italic_V</annotation></semantics></math>). Then, it requires computing the similarity in scores between the query and keys via matrix multiplication (<math alttext="QK^{T}" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.4.m4.1"><semantics id="S2.SS1.SSS1.p1.4.m4.1a"><mrow id="S2.SS1.SSS1.p1.4.m4.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.SSS1.p1.4.m4.1.1.2" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS1.p1.4.m4.1.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.1.cmml">‚Å¢</mo><msup id="S2.SS1.SSS1.p1.4.m4.1.1.3" xref="S2.SS1.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.SSS1.p1.4.m4.1.1.3.2" xref="S2.SS1.SSS1.p1.4.m4.1.1.3.2.cmml">K</mi><mi id="S2.SS1.SSS1.p1.4.m4.1.1.3.3" xref="S2.SS1.SSS1.p1.4.m4.1.1.3.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.4.m4.1b"><apply id="S2.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1"><times id="S2.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.1"></times><ci id="S2.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2">ùëÑ</ci><apply id="S2.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS1.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.3.2">ùêæ</ci><ci id="S2.SS1.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.3.3">ùëá</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.4.m4.1c">QK^{T}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.4.m4.1d">italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>), which is then scaled by the square root of the key dimensions to stabilize gradients during training. Masking can be applied to selectively ignore specific parts of the input, such as future tokens, in sequence prediction tasks. To produce attention weights, the similarity scores are normalized by using the softmax function. This also highlights the importance of each key relative to the query. The Values (<math alttext="V" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.5.m5.1"><semantics id="S2.SS1.SSS1.p1.5.m5.1a"><mi id="S2.SS1.SSS1.p1.5.m5.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.5.m5.1b"><ci id="S2.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.5.m5.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.5.m5.1d">italic_V</annotation></semantics></math>) are then combined with the weighted sum, which is guided by these attention weights. The Multi-head attention model works like an extension by running multiple attention computations in parallel, capturing diverse contextual relationships within the system.</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S2.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of the core mechanisms of modern deep learning frameworks: (a) Attention, including scaled dot-product and multi-head attention (b) Graph Attention for processing graph-based data, and (c) Transformer architecture with encoder-decoder structure.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS2.4.1.1">II-A</span>2 </span>Graph Attention</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Graph Attention Networks (GATs) are a neural network architecture specifically engineered for graph-structured data, employing the attention mechanism to overcome the shortcomings of previous graph neural networks (GNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib19" title="">19</a>]</cite>. GATs utilize self-attention layers rather than fixed convolution procedures, enabling each node to concentrate on its most pertinent neighbors selectively. This is accomplished by calculating attention scores that signify the significance of adjacent nodes according to their attributes. The model subsequently consolidates these attributes, weighted by the attention ratings, to enhance the node‚Äôs representation.
GATs do not require the complete graph structure in advance and may accommodate variable-sized neighborhoods, rendering them quite adaptable. Moreover, multi-head attention captures many dimensions of node interactions, augmenting the network‚Äôs ability to understand intricate patterns.
GATs have demonstrated robust efficacy in node categorization, connection prediction, and community discovery across several domains, including social networks, biological networks, and citation graphs.
Furthermore, GATs are useful for assessing circuit interdependence and finding possible flaws. By conceptualizing circuit parts (e.g., gates, flip-flops) as nodes and their interactions as edges, GATs can model whole circuits.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F5" title="Figure 5 ‚Ä£ II-A1 Attention Mechanism ‚Ä£ II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">5</span></a>-(b) illustrates the overview of the graph attention mechanism, which is the heart of GATs. The figure showcases how each node computes attention scores to focus on their most relevant neighbors by employing this attention mechanism. It does this specifically by computing attention coefficients (<math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.1.m1.1"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mi id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><ci id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.1.m1.1d">italic_Œ±</annotation></semantics></math>), which signifies the importance of each neighboring node based on features (<math alttext="h_{1},h_{2},h_{3},..." class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.2.m2.4"><semantics id="S2.SS1.SSS2.p2.2.m2.4a"><mrow id="S2.SS1.SSS2.p2.2.m2.4.4.3" xref="S2.SS1.SSS2.p2.2.m2.4.4.4.cmml"><msub id="S2.SS1.SSS2.p2.2.m2.2.2.1.1" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1.cmml"><mi id="S2.SS1.SSS2.p2.2.m2.2.2.1.1.2" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1.2.cmml">h</mi><mn id="S2.SS1.SSS2.p2.2.m2.2.2.1.1.3" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS2.p2.2.m2.4.4.3.4" xref="S2.SS1.SSS2.p2.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS1.SSS2.p2.2.m2.3.3.2.2" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2.cmml"><mi id="S2.SS1.SSS2.p2.2.m2.3.3.2.2.2" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2.2.cmml">h</mi><mn id="S2.SS1.SSS2.p2.2.m2.3.3.2.2.3" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS2.p2.2.m2.4.4.3.5" xref="S2.SS1.SSS2.p2.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS1.SSS2.p2.2.m2.4.4.3.3" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3.cmml"><mi id="S2.SS1.SSS2.p2.2.m2.4.4.3.3.2" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3.2.cmml">h</mi><mn id="S2.SS1.SSS2.p2.2.m2.4.4.3.3.3" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3.3.cmml">3</mn></msub><mo id="S2.SS1.SSS2.p2.2.m2.4.4.3.6" xref="S2.SS1.SSS2.p2.2.m2.4.4.4.cmml">,</mo><mi id="S2.SS1.SSS2.p2.2.m2.1.1" mathvariant="normal" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml">‚Ä¶</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.4b"><list id="S2.SS1.SSS2.p2.2.m2.4.4.4.cmml" xref="S2.SS1.SSS2.p2.2.m2.4.4.3"><apply id="S2.SS1.SSS2.p2.2.m2.2.2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p2.2.m2.2.2.1.1.2.cmml" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1.2">‚Ñé</ci><cn id="S2.SS1.SSS2.p2.2.m2.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.SSS2.p2.2.m2.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.SSS2.p2.2.m2.3.3.2.2.cmml" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.2.m2.3.3.2.2.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS1.SSS2.p2.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2.2">‚Ñé</ci><cn id="S2.SS1.SSS2.p2.2.m2.3.3.2.2.3.cmml" type="integer" xref="S2.SS1.SSS2.p2.2.m2.3.3.2.2.3">2</cn></apply><apply id="S2.SS1.SSS2.p2.2.m2.4.4.3.3.cmml" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.2.m2.4.4.3.3.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p2.2.m2.4.4.3.3.2.cmml" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3.2">‚Ñé</ci><cn id="S2.SS1.SSS2.p2.2.m2.4.4.3.3.3.cmml" type="integer" xref="S2.SS1.SSS2.p2.2.m2.4.4.3.3.3">3</cn></apply><ci id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1">‚Ä¶</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.4c">h_{1},h_{2},h_{3},...</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.2.m2.4d">italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , ‚Ä¶</annotation></semantics></math>). These coefficients are also used in generating updated representation (<math alttext="h^{\prime}_{1}" class="ltx_Math" display="inline" id="S2.SS1.SSS2.p2.3.m3.1"><semantics id="S2.SS1.SSS2.p2.3.m3.1a"><msubsup id="S2.SS1.SSS2.p2.3.m3.1.1" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml"><mi id="S2.SS1.SSS2.p2.3.m3.1.1.2.2" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.2.cmml">h</mi><mn id="S2.SS1.SSS2.p2.3.m3.1.1.3" xref="S2.SS1.SSS2.p2.3.m3.1.1.3.cmml">1</mn><mo id="S2.SS1.SSS2.p2.3.m3.1.1.2.3" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.3.cmml">‚Ä≤</mo></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.3.m3.1b"><apply id="S2.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1">subscript</csymbol><apply id="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p2.3.m3.1.1.2.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1">superscript</csymbol><ci id="S2.SS1.SSS2.p2.3.m3.1.1.2.2.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.2">‚Ñé</ci><ci id="S2.SS1.SSS2.p2.3.m3.1.1.2.3.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.3">‚Ä≤</ci></apply><cn id="S2.SS1.SSS2.p2.3.m3.1.1.3.cmml" type="integer" xref="S2.SS1.SSS2.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.3.m3.1c">h^{\prime}_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS2.p2.3.m3.1d">italic_h start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>) for the target node. As shown in this figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F5" title="Figure 5 ‚Ä£ II-A1 Attention Mechanism ‚Ä£ II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">5</span></a>-(b), the process also involves the multiple attention heads that compute various perspectives of node interactions. This approach enhances the model‚Äôs ability by identifying the complex patterns in graph-structured data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS3.4.1.1">II-A</span>3 </span>Transformer</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The transformer is a neural network architecture that uses self-attention mechanisms, eliminating traditional recurrent and convolutional layers. It consists of an encoder and a decoder, each built with layers of self-attention and feed-forward networks. The self-attention mechanism allows the model to capture relationships across entire input sequences in a highly parallelizable manner, improving computational efficiency. Multi-head attention enables the model to focus on multiple input segments simultaneously, enhancing tasks like machine translation and text generation. The encoder-decoder transformer is ideal for sequence-to-sequence applications, such as machine translation. The encoder in this architecture converts the input sequence into a context vector, which the decoder uses to generate the output sequence. Encoder-based transformers, such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib20" title="">20</a>]</cite>, are designed for tasks requiring an understanding of entire input sequences, like text classification and entity recognition. Decoder-based transformers, such as GPT, specialize in sequence generation, predicting output tokens sequentially in an autoregressive manner by analyzing relationships among previously generated tokens.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">The architecture, illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F5" title="Figure 5 ‚Ä£ II-A1 Attention Mechanism ‚Ä£ II-A AI and Machine Learning ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">5</span></a>-(c), consists of encoder and decoder modules with self-attention and feed-forward layers. The encoder captures contextual relationships within input sequences, while the decoder generates output sequences autoregressive. Transformers architecture also supports hardware design and security by enabling efficient sequence modeling, dependency extraction, circuit representation learning, Verilog code generation, vulnerability identification, and mitigation generation in hardware designs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Hardware Design and Challenges </span>
</h3>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="126" id="S2.F6.g1" src="x6.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Overview of the key stages in the globalized IC design flow.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The hardware design flow is a systematic process that converts high-level design specifications into a functioning integrated circuit ready for implementation, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F6" title="Figure 6 ‚Ä£ II-B Hardware Design and Challenges ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">6</span></a>. The process starts by delineating functional and performance criteria in the design specification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib2" title="">2</a>]</cite>. The design team incorporates third-party intellectual property (IP) blocks to accelerate development. The design is captured at the RTL using HDLs such as Verilog or VHDL, emphasizing functionality. The RTL description undergoes synthesis into a gate-level netlist through logic synthesis, which translates the design into fundamental logic gates while optimizing for performance, power, and area. This netlist undergoes physical synthesis, encompassing placement, and routing, to produce a comprehensive physical layout. The design is manufactured onto silicon wafers using sophisticated semiconductor techniques, such as photolithography, ion implantation, and chemical vapor deposition, to precisely pattern and build the intricate layers of the integrated circuit. Upon fabrication, the wafers are encased for protection and seamless integration, followed by stringent testing to verify compliance with functional and performance standards. The design team delivers the evaluated integrated circuits as the final product for incorporation into electronic systems. This rigorous process ensures the attainment of design objectives while minimizing mistakes and inefficiencies.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The advancement in hardware design is progressing exponentially, leading to significant and unpredictable challenges that require continuous attention. Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F7" title="Figure 7 ‚Ä£ II-B Hardware Design and Challenges ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the key challenges in the hardware design process, which include power efficiency, scalability, performance optimization, signal integrity, timing analysis, verification and validation, physical design constraints, and time-to-market pressures.
Modern Integrated Circuits (ICs) integrate billions of transistors with intricate interdependencies, making seamless functionality and scalability critical challenges. This increasing complexity has driven the development of System-on-Chip (SoC) architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib21" title="">21</a>]</cite>, which address these challenges by compacting designs and enhancing performance. However, SoCs introduce new difficulties in verification and validation, as traditional methods struggle to handle the vast number of components and interactions within a single chip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib10" title="">10</a>]</cite>. Power management is a critical focus, especially with the proliferation of IoT and mobile devices. Designers must innovate to reduce power consumption and manage heat dissipation without compromising performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib23" title="">23</a>]</cite>. Performance optimization remains a primary goal in applications such as AI accelerators and high-speed communication systems, where achieving high-speed operations, low latency, and superior throughput are essential <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib24" title="">24</a>]</cite>. Signal integrity and timing analysis are equally vital. As signal paths shrink and frequencies rise in advanced nodes, ensuring reliable data transmission and synchronization has become increasingly challenging. Static Timing Analysis (STA) is commonly employed, but its accuracy is often hindered by the growing complexity and miniaturization of components <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib27" title="">27</a>]</cite>. Physical design challenges, such as floor planning, layout optimization, and routing, further complicate the hardware development process. Efficiently arranging chip components with proper routing to balance performance, power consumption, and thermal management is critical <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib28" title="">28</a>]</cite> and necessitates advanced algorithms and tools. Verification and validation have become increasingly resource-intensive and time-consuming, demanding advanced techniques to ensure all modules adhere to specifications and function as intended. Physical constraints in design also impact manufacturability and yield, requiring careful optimization of layouts and static timing. Time-to-market pressures add to these challenges, as delays in addressing any issue can lead to significant market disadvantages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib29" title="">29</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="690" id="S2.F7.g1" src="x7.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Key challenges in hardware design.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">ML can change traditional hardware design by increasing efficiency and accuracy at each stage of the workflow and by helping to solve challenges such as scalability, optimization, and decision-making that otherwise involve considerable manual effort and computational resources.
The selection and integration of third-party IPs can be optimized through ML models in the design specification and integration stage using compatibility and performance metrics prediction. Predictive ML techniques, for instance, analyze design requirements to provide suitable recommendations for IPs by reducing integration errors and improving design efficiency. Tools based on ML, such as ChipGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib30" title="">30</a>]</cite>, already help designers at the RTL design stage with power-performance-area-optimized Verilog code. Similarly, ML models have been trained on hardware datasets to detect and repair logical errors in HDL designs, which helps enhance design reliability and scalability.
ML algorithms enhance optimization during logic synthesis and gate-level netlist generation by translating HDL into efficient gate-level netlists. In this respect, reinforcement learning methods optimize adaptive constraints for balanced timing, power, and area requirements¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib31" title="">31</a>]</cite>. ML significantly improves physical synthesis and layout, especially for placement and routing. Gao et al. demonstrated using graph-based neural networks to predict congestion hotspots and optimize wire length, reducing timing violations and ensuring manufacturability<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib32" title="">32</a>]</cite>. Additionally, convolutional neural networks (CNNs) accelerate power grid performance prediction, offering faster evaluations than traditional methods.
Additionally, ML can be to testing and validation to automate test pattern generation and improve fault detection. Advanced ML models analyze test data to predict future failures, optimizing overall testing and fault coverage. These ML-driven improvements meet demands for complexity, scalability, and performance in modern hardware design with significantly reduced manual effort throughout the flow.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Hardware Security and Challenges</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The evolution of hardware in recent years has introduced numerous security challenges that demand innovative solutions. Significant hardware challenges prevalent today include Trojans, IP theft, side-channel attacks, fault injection attacks, reverse engineering, and supply chain attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib36" title="">36</a>]</cite>. The globalization of the hardware supply chain has exacerbated these problems, necessitating robust detection and mitigation solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib37" title="">37</a>]</cite>. Hardware Trojans are malicious and stealthy circuit alterations that compromise device integrity and functionality. The insertion of hardware Trojans is one of the most significant security challenges in hardware security, reported as one of the stealthiest forms of cyber attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib25" title="">25</a>]</cite>. Another critical attack is the side-channel attack, where attackers exploit circuit leakages, such as power consumption and EM emissions, to extract sensitive information from the system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib38" title="">38</a>]</cite>. Fault injection attacks, where faults are deliberately induced in the circuit, represent another form of hardware attack, leading to data corruption and unauthorized access to the device‚Äôs data and systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib39" title="">39</a>]</cite>. Additionally, ICs are vulnerable to reverse engineering, which involves analyzing the IC to uncover its design and functions, preventing unauthorized replications, and identifying design security vulnerabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2.F8" title="Figure 8 ‚Ä£ II-C Hardware Security and Challenges ‚Ä£ II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">8</span></a> outlines key hardware security threats across the IC design lifecycle, including hardware Trojans, side-channel attacks, fault injection, and reverse engineering.
Attackers try to exploit the IC for unintended leakages, such as power consumption or electromagnetic emissions, while the IC operates.
In fault injection attacks, the attacker deliberately induces faults to corrupt data and disrupt systems.
Conversely, reverse engineering attacks can apply to deployed chips. Such attacks allow the attacker to reconstruct designs and extract proprietary information from ICs during the testing and packaging stages.</p>
</div>
<figure class="ltx_figure" id="S2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="291" id="S2.F8.g1" src="x8.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Overview of key hardware security threats and challenges in modern hardware design.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">ML for hardware security has emerged as a research subject and effectively mitigates complex security challenges. One key area where ML techniques excel is hardware Trojan detection, with models trained to identify anomalies in circuit behavior that may indicate malicious modifications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib41" title="">41</a>]</cite>. Studies have shown that ML algorithms analyze power and other side-channel data to detect discrepancies caused by hardware Trojans<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib42" title="">42</a>]</cite>. Additionally, ML-based techniques effectively mitigate side-channel vulnerabilities by understanding and reducing leakage patterns, enhancing the system‚Äôs overall security posture<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib46" title="">46</a>]</cite>. Logic obfuscation is another critical area in hardware security that uses ML to enhance circuit design security by introducing transformations that make reverse engineering significantly harder<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib47" title="">47</a>]</cite>. Surveys further highlight ML‚Äôs role in both physical and logic testing techniques for hardware Trojan detection<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib49" title="">49</a>]</cite>. ML also extends its applications to fault injection attack prevention and supply chain security, providing scalable, automated, and efficient solutions to reinforce hardware security<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib52" title="">52</a>]</cite>. By leveraging ML‚Äôs capabilities, researchers and engineers are developing robust countermeasures to protect against a wide variety of hardware security threats.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Attention-Based Hardware design</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S2" title="II Background and Motivation ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">II</span></a>, we covered the current challenges in hardware design. Now, we will delve into the LLM-based and other attention-based solutions identified for these challenges and examine ongoing research into new technologies to provide better solutions.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>LLM-based Digital Hardware Design </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:551.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-184.3pt,234.2pt) scale(0.540588036146517,0.540588036146517) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1.1" style="background-color:#E3E3E3;">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1" style="background-color:#E3E3E3;">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.2">
<span class="ltx_inline-block" id="S3.T1.1.1.1.1.2.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S3.T1.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1.1.1">Method</span></span>
<span class="ltx_p" id="S3.T1.1.1.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1.2.1">Name</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.3">
<span class="ltx_inline-block" id="S3.T1.1.1.1.1.3.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S3.T1.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1.1.1">Design</span></span>
<span class="ltx_p" id="S3.T1.1.1.1.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1.2.1">Assistance</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.4">
<span class="ltx_inline-block" id="S3.T1.1.1.1.1.4.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S3.T1.1.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1.1.1">Optimization</span></span>
<span class="ltx_p" id="S3.T1.1.1.1.1.4.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1.2.1">Synthesis</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.5">
<span class="ltx_inline-block" id="S3.T1.1.1.1.1.5.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S3.T1.1.1.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.5.1.1.1">Verification</span></span>
<span class="ltx_p" id="S3.T1.1.1.1.1.5.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.5.1.2.1">Validation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.6">
<span class="ltx_inline-block" id="S3.T1.1.1.1.1.6.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S3.T1.1.1.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.6.1.1.1">Debugging</span></span>
<span class="ltx_p" id="S3.T1.1.1.1.1.6.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.6.1.2.1">Maintenance</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.7.1" style="background-color:#E3E3E3;">AI Model</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.1">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib53" title="">53</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.7">
<span class="ltx_text" id="S3.T1.1.1.2.2.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.2.2.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.2.2.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.2.2.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.2.2.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.2.2.7.2.1.1.1.1" style="font-size:80%;">CodeGen(2B, 6B, 16B), MegatronLM-355M,</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.2.2.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.2.2.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.2.2.7.2.1.2.1.1" style="font-size:80%;">Code-davinci-002</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.2.2.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.2">ChipChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib8" title="">8</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.7">ChatGPT-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.2">VerilogEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.7">GPT(3.5, 4), CodeGen-16B-verilog</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.2">RTLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.7">GPT-3.5</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib54" title="">54</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.3">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.4">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.6.6.7">GPT(3.5, 4)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.2">RTLFixer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib39" title="">39</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.6">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7.7">GPT(3.5, 4)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.2">RTLCoder 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib38" title="">38</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.8.8.7">Mistral-7B, DeepSeek-Coder-6.7B</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.2">AutoChip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib9" title="">9</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.9.9.7">GPT(3.53.5-turbo, 4), Claude 2, Code Llama 2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.2">ShortCircuit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib55" title="">55</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.10.10.7">ChatGPT</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.2">VeriGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib56" title="">56</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.11.11.7">
<span class="ltx_text" id="S3.T1.1.1.11.11.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.11.11.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.11.11.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.11.11.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.11.11.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.11.11.7.2.1.1.1.1" style="font-size:80%;">MegatronLM-355M, CodeGen(2B, 6b, 16b),</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.11.11.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.11.11.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.11.11.7.2.1.2.1.1" style="font-size:80%;">J1-Large-7B, code-davinci-002, GPT(3.5-turbo, 4),</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.11.11.7.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.11.11.7.2.1.3.1"><span class="ltx_text" id="S3.T1.1.1.11.11.7.2.1.3.1.1" style="font-size:80%;">PALM2</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.11.11.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.2">LLM4EDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib11" title="">11</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.12.12.7">GPT(3.5, 4), LLaMA2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.2">SpecLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib57" title="">57</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.13.13.7">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.2">AssertLLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.14.14.7">GPT(3.5, 4, 4-Turbo)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.15.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.2">ChIRAAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib58" title="">58</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.15.15.7">GPT-4 Turbo</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.16.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib59" title="">59</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.16.16.7">VeriGen-2B (Fine-tuned version of CodeGen LLM)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.17.17">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib60" title="">60</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.17.17.7">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib2" title="">2</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.18.18.7">ChatGPT-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.19.19">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.19.19.7">GPT-4, LLaMA2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.20.20">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.2">HDLdebugger <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib62" title="">62</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.6">‚úì</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.20.20.7">
<span class="ltx_text" id="S3.T1.1.1.20.20.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.20.20.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.20.20.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.20.20.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.20.20.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.20.20.7.2.1.1.1.1" style="font-size:80%;">GPT-4, RTLFixer, VeriGen,</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.20.20.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.20.20.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.20.20.7.2.1.2.1.1" style="font-size:80%;">CodeLlama-13b (base model for HDLdebugger)</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.20.20.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.21.21">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib63" title="">63</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.6">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.21.21.7">Llama 2-13B, Llama 2-7B</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.22.22">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.2">ChatPattern<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib64" title="">64</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.22.22.7"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.23.23">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib65" title="">65</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.6">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.23.23.7">GPT(3.5-Turbo, 4)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.24.24">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib66" title="">66</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.24.24.7">
<span class="ltx_text" id="S3.T1.1.1.24.24.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.24.24.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.24.24.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.24.24.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.24.24.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.24.24.7.2.1.1.1.1" style="font-size:80%;">GPT(3.5, 4), CodeLlama(7B, 13B),</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.24.24.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.24.24.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.24.24.7.2.1.2.1.1" style="font-size:80%;">VeriGen(6B, 16B)</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.24.24.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.25.25">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib67" title="">67</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.25.25.7">
<span class="ltx_text" id="S3.T1.1.1.25.25.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.25.25.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.25.25.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.25.25.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.25.25.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.25.25.7.2.1.1.1.1" style="font-size:80%;">GPT(3.5, 4), Claude,</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.25.25.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.25.25.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.25.25.7.2.1.2.1.1" style="font-size:80%;">Gemini Advanced</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.25.25.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.26.26">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib68" title="">68</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.26.26.7">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.27.27">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib69" title="">69</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.5">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.6">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.27.27.7">ChatGPT-4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.28.28">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib70" title="">70</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.28.28.7">ChatGPT(3.5, 4), Bard, HuggingChat</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.29.29">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.2">RTLCoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib71" title="">71</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.29.29.7">
<span class="ltx_text" id="S3.T1.1.1.29.29.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.29.29.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.29.29.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.29.29.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.29.29.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.29.29.7.2.1.1.1.1" style="font-size:80%;">RTLCoder (based on Mistral-7B and</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.29.29.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.29.29.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.29.29.7.2.1.2.1.1" style="font-size:80%;">DeepSeek-Coder-6.7B)</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.29.29.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.30.30">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib72" title="">72</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.30.30.7">CodeGen(2B, 6B, 16B), GEMMA(2B, 7B)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.31.31">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib73" title="">73</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.6">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.31.31.7">GPT-4, Codex, CodeGen</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.32.32">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib74" title="">74</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.32.32.7">Hardware Phi-1.5B</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.33.33">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib75" title="">75</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.3">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.4">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.33.33.7">Microsoft Bing Chat</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.34.34">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib76" title="">76</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.34.34.7">Llama2-70B</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.35.35">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib77" title="">77</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.35.35.7">GPT(4, 4V), LLaVA, LLaMA</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.36.36">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib78" title="">78</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.5">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.6">‚úì</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.36.36.7">
<span class="ltx_text" id="S3.T1.1.1.36.36.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.36.36.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.36.36.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.36.36.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.36.36.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.36.36.7.2.1.1.1.1" style="font-size:80%;">GPT-3.5, CodeLlama, DeepSeekCoder, CodeQwen</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.36.36.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.37.37">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib79" title="">79</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.3">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.37.37.7">GPT(3.5, 4)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.38.38">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib80" title="">80</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.38.38.7">GPT(3.5, 4, 4o)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.39.39">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib81" title="">81</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.39.39.7">AutoMage (fine-tuned on Llama2-70B), GPT(3.5, 4)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.40.40">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib82" title="">82</a>]</cite></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.40.40.7">GPT-3.5-turbo-16k-0613</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.41.41">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib83" title="">83</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.41.41.7">
<span class="ltx_text" id="S3.T1.1.1.41.41.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.41.41.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.41.41.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.41.41.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.41.41.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.41.41.7.2.1.1.1.1" style="font-size:80%;">Codellama-7B, DeepSeek-Coder-6.7B,</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.41.41.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.41.41.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.41.41.7.2.1.2.1.1" style="font-size:80%;">CodeQwen1.5-7B</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.41.41.7.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.42.42">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib84" title="">84</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.3">‚úì</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.4"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.5"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.6"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.42.42.7">
<span class="ltx_text" id="S3.T1.1.1.42.42.7.1"></span> <span class="ltx_text" id="S3.T1.1.1.42.42.7.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.42.42.7.2.1">
<span class="ltx_tr" id="S3.T1.1.1.42.42.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.42.42.7.2.1.1.1"><span class="ltx_text" id="S3.T1.1.1.42.42.7.2.1.1.1.1" style="font-size:80%;">Llama 2, Code Llama, VeriGen,</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.42.42.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.42.42.7.2.1.2.1"><span class="ltx_text" id="S3.T1.1.1.42.42.7.2.1.2.1.1" style="font-size:80%;">CL-Verilog, RTL-Coder, Llama 3,</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.42.42.7.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.42.42.7.2.1.3.1"><span class="ltx_text" id="S3.T1.1.1.42.42.7.2.1.3.1.1" style="font-size:80%;">GPT(3.5, 4)</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.42.42.7.3"></span></th>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">LLM-based Digital Hardware Design</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">This section explores the research that used LLMs for hardware design using various techniques.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.F9" title="Figure 9 ‚Ä£ III-A1 Design And Specification Assistance ‚Ä£ III-A LLM-based Digital Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates the integrated overview of the critical applications of LLMs in digital hardware design. It indicates the processes in the hardware design flow that benefit most from LLMs. This paper categorizes the design process into four major aspects: design and specification assistance, optimization and synthesis, verification and validation, and debugging and maintenance. Each section indicates distinct hardware stages where LLMs ensure seamless integration, automation, and enhancement across these sections. LLMs are capable of supporting tasks such as specification drafting, Verilog code generation, performance optimization, testbench generation, and error detection. This figure provides a comprehensive overview, setting the stage for a deeper examination of each domain in the subsections that follow.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.4.1.1">III-A</span>1 </span>Design And Specification Assistance</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">In this category, LLMs aid in the initial stages of hardware design by drafting design specifications and converting natural language descriptions into formal HDLs.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.F10" title="Figure 10 ‚Ä£ III-A1 Design And Specification Assistance ‚Ä£ III-A LLM-based Digital Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">10</span></a> illustrates the comprehensive workflow of hardware design using LLMs, demonstrating their role in automating and enhancing various stages of the IC design process. The process begins with a human prompting in natural language, and then, depending on the type of LLMs, the output files as Verilog code are generated. This process enables efficient design and specification assistance in the chip design process. The generated Verilog codes are then synthesized and proceed to fabrication, with the verification steps in between to ensure correctness and compliance with performance requirements. The iterative feedback loop between the verification and synthesis stages highlights how LLMs optimize the overall design flow by reducing errors and improving efficiency at each stage of the design timeline.</p>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F9.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Overview of the critical applications of LLMs in digital hardware design, encompassing specification drafting, optimization, verification, and debugging to streamline the design lifecycle. Each application category includes key functionalities such as specification drafting, PPA optimization, testbench generation, and error correction</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">To support this, a significant body of research has focused on leveraging LLMs for Verilog code generation. Thakur et al. pioneered this area by evaluating the ability of LLMs to generate syntactically correct and functionally accurate Verilog RTL code by fine-tuning models on a large Verilog corpus and benchmarking their performance with the fine-tuned CodeGen-16B model, achieving the highest accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib53" title="">53</a>]</cite>. In another similar research, Thakur et al. compiled the largest corpus of Verilog code from different open-source repositories, which was then used to fine-tune an LLM. The LLM model called VeriGen was then used for HDL generation, and the results were evaluated against the advanced general-purpose LLMs regarding the correctness of the code generated. The performance of the fine-tuned model was found to be significantly improved <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib56" title="">56</a>]</cite>. In another research endeavor, Thakur et al. introduced Autochip, a fully automated feedback-driven verilog code generation tool that iteratively refines designs based on the compiler output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Further studies have expanded the exploration of LLMs in functional HDL generation. Yang et al. further highlighted the potential of LLMs by using GPT-4 for generating functional Verilog code for complex designs like systolic arrays and AI accelerators for ResNet and MobileNet. The approach yielded results; however, it required significant manual optimization, especially for complex designs ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib1" title="">1</a>]</cite>. Similarly, Yanƒ±k et al. introduced ShortCircuit, a tool that automates front-end digital integrated circuit design using ChatGPT and OpenLane. ShortCircuit significantly reduced design time for ASIC and FPGA implementations by generating HDL, GDS, and bitstream files¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib55" title="">55</a>]</cite>.In the same way, Tomlinson et al. leveraged ChatGPT to generate synthesizable and functional Verilog descriptions for a programmable Spiking Neuron Array ASIC through natural language prompts, validating the design through simulation and preparing it for fabrication using Skywater 130nm technology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib2" title="">2</a>]</cite>.
Efforts have also focused on enhancing dataset quality and benchmarking. Chang et al., in the ‚ÄúData is all you need,‚Äù paper, proposed an automated data augmentation framework to generate high-quality verilog and EDA script generation. By converting Verilog files into abstract syntax trees, applying predefined repair rules, and leveraging GPT-3.5 for EDA script descriptions, the framework generates high-quality datasets. When the Llama 2-7B and Llama 2-13B models were fine-tuned with these data, a significant improvement in accuracy was achieved. The author used the Thakur et al. benchmark and achieved an 11. 8% improvement in the pass rate compared to Thakur et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="639" id="S3.F10.g1" src="x10.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Workflow of hardware design using LLMs, highlighting how both prompting with pre-trained models and fine-tuning with Verilog sources and training data enable the complete hardware design process.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">Expanding beyond single-model fine-tuning, Several studies have focused on fine-tuning and customizing LLMs for hardware design tasks. Liu et al. presented a lightweight, customized, and fine-tuned LLM called RTLCoder that outperforms GPT-3.5. They quantized the RTLCoder to 4 bits, making the model lightweight and compatible with running on a normal laptop as a local assistant to hardware engineers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib71" title="">71</a>]</cite>.In parallel, Nadimi and Zheng introduced the Multi-Expert Verilog LLM (MEV-LLM) architecture, which leveraged multiple LLMs fine-tuned with datasets categorized by design complexity to enhance Verilog code generation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib72" title="">72</a>]</cite>. By developing a categorized dataset with fine-grained annotations, the MEV-LLM shows up to a 23.9% improvement in generating syntactically and functionally correct Verilog code compared to state-of-the-art models. Evaluated on tasks from the HDLBits website, the architecture demonstrates significant performance gains, emphasizing the effectiveness of multi-expert models in addressing various design complexities.
A similar goal was pursued by Gao et al. through AutoVCoder, which implemented a retrieval-augmented generation (RAG) mechanism for enhancing the syntactic and functional correctness of Verilog code generation by LLMs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib83" title="">83</a>]</cite>. AutoVCoder outperformed existing LLMs, showing significant improvements in syntax and functional correctness across multiple benchmarks. The first fine-tuning round focused on basic Verilog syntax, while the second addressed realistic problem-solving tasks. The RAG module integrated examples and knowledge retrievers, improving accuracy and addressing hallucination issues. The study highlighted AutoVCoder as a robust and efficient methodology for Verilog code generation.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1">To further refine HDL generation capabilities, Nakkab et al. explored the use of hierarchical prompting techniques to improve the effectiveness of LLMs in handling complex hardware design tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib84" title="">84</a>]</cite>. The study introduced a suite of hierarchical prompting techniques, facilitating stepwise design methods for efficient HDL code generation. They developed a generalizable automation pipeline and presented a new benchmark set of hardware designs with hierarchical solutions. Empirical evaluations demonstrated that hierarchical prompting significantly improved performance, particularly for complex designs, by breaking down tasks into manageable submodules. Notably, they successfully generated a 16-bit MIPS processor and a 32-bit RISC-V processor using purely generative hierarchical prompting. The study concludes that hierarchical prompting is a crucial tool for automated HDL generation, allowing smaller LLMs to compete with larger models.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p6">
<p class="ltx_p" id="S3.SS1.SSS1.p6.1">Another line of research investigates the interface between LLMs and design environments. Abdelatty and Reda introduced HDLCopilot, an LLM-powered query system designed to interact with Process Design Kits (PDKs) using natural language, significantly enhancing the efficiency and accuracy of hardware design workflows¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib80" title="">80</a>]</cite>. HDLCopilot converts natural language queries into SQL queries to retrieve relevant data from a PDK database, providing natural language responses. The system comprises four agents: Dispatcher, Selector, SQL Generator, and Interpreter, each handling different aspects of query processing. Evaluated on the Skywater 130nm PDK database, HDLCopilot achieved high accuracy (94.23%) and efficiency (98.07%) in information retrieval. This multi-agent framework demonstrated the ability to handle complex queries and streamline the design and verification processes, showing promise for further integration with hardware design tools and enhanced training schemes to support diverse and complex PDK queries.Futhermore, Chang et al. introduced a novel approach leveraging multimodal generative AI that combines natural language and visual input to improve Verilog code generation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib77" title="">77</a>]</cite>. The study highlights the limitations of purely natural language-based models in capturing complex spatial relationships and proposes a benchmark for evaluating multi-modal models. The results show substantial improvements in syntax correctness and functionality, with test bench passing rates for GPT-4 models improving from 46.88% to 71.81% and LLaMA models from 13.41% to 25.88%. The research underscores the advantages of multimodal input in generating accurate Verilog code, particularly for multimodule hardware and state machines. The authors are working to refine multimodal models, develop more comprehensive benchmarks, and improve their integration with EDA tools, ultimately improving hardware design processes.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p7">
<p class="ltx_p" id="S3.SS1.SSS1.p7.1">Recent works have also explored broader applications. Mengming et al. showed LLMs‚Äô potential to enhance IC development by streamlining the creation of architecture specifications, from high-level to detailed descriptions, using specific prompts that integrate RTL code ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib57" title="">57</a>]</cite>. Their text analysis capabilities also provide effective overviews and unique insights, reducing human error and saving time. Wang et al. introduced ChatPattern, a framework powered by LLMs for generating and customizing layout patterns based on natural language inputs. ChatPattern leveraged an LLM agent capable of interpreting user requirements, decomposing them into subtasks, and autonomously operating design tools to generate layout patterns¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib64" title="">64</a>]</cite>. The framework performs tasks such as conditional layout generation, pattern modification, and both fixed-size and free-size pattern generation. Experimental results highlight ChatPattern‚Äôs performance in producing diverse and legally compliant patterns compared to existing methods. The study underscores the potential of ChatPattern to provide a user-friendly interface for the customization of layout patterns, although future research is needed to improve the handling of complex and oversized patterns and to enhance global guidance during pattern generation.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p8">
<p class="ltx_p" id="S3.SS1.SSS1.p8.1">Liu et al. introduced a tool leveraging the Llama2-70B language model to automate the layout of silicon photonic devices, enhancing photonic design automation (PDA)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib76" title="">76</a>]</cite>. This tool significantly reduces human intervention by generating layout code from natural language prompts. Preliminary tests showed a success rate of over 99% in generating accurate layout code. Direct comparisons with GPT-4 highlighted superior performance in error-free code generation. The tool was tested with various user prompts, consistently producing accurate layouts. It used system prompts, SQL database checks, and code verification to ensure accuracy. This tool shows potential for automating silicon photonic device layout with high efficiency and minimal human intervention, though further development is needed to handle more complex tasks such as routing and component placement.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p9">
<p class="ltx_p" id="S3.SS1.SSS1.p9.1">He et al. introduced ChatEDA, an autonomous agent for EDA powered by the AutoMage large language model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib81" title="">81</a>]</cite>. ChatEDA streamlines the EDA design flow from RTL to Graphic Data System Version II (GDSII) by managing task planning, script generation, and task execution. AutoMage, fine-tuned using a self-instruction paradigm and QLoRA technique, outperformed other models like GPT-4 in task planning and script generation. Evaluations showed that AutoMage achieved a Grade A in 88% of test cases compared to 58% for GPT-4, demonstrating its effectiveness in automating the RTL-to-GDSII design flow. The study highlighted the potential of ChatEDA to enhance design efficiency and reduce errors, though further improvements are needed for more complex design scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p10">
<p class="ltx_p" id="S3.SS1.SSS1.p10.1">To evaluate creativity in hardware design, DeLorenzo et al. introduced CreativEval, a framework for assessing LLM generated design based on fluency, flexibility, originality, and elaboration. They evaluated various LLMs, including GPT models, CodeLlama, and VeriGen, demonstrating GPT-3.5 emerging as the most creative.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib66" title="">66</a>]</cite>. Complementarily, Zhao et al. introduced CodeV, a framework enhancing LLMs for Verilog code generation through multi-level summarization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib78" title="">78</a>]</cite>. By curating high-quality GitHub data and leveraging GPT-3.5, it improves instruction-tuning datasets and outperforms GPT-4 and BetterV on benchmarks. Fine-tuning models like CodeLlama boosts performance, demonstrating its potential for circuit design automation.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p11">
<p class="ltx_p" id="S3.SS1.SSS1.p11.1">HLS tools facilitate rapid hardware design from C code but face limitations due to incompatible code constructs. In this paper, Collini et al. investigate the use of LLMs to refactor C code into HLS-compatible formats. Several case studies are presented where an LLM rewrites C code for NIST 800-22 randomness tests, QuickSort, and AES-128 into HLS-synthesizable C. The LLM iteratively transforms the code with user guidance, adding functions like streaming data and hardware-specific signals. This study highlights the LLM‚Äôs potential in aiding the refactoring of standard C code into HLS-synthesizable code¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib85" title="">85</a>]</cite>. similarly, Meech introduced a methodology combining HLS tools and LLMs to generate, simulate, and deploy hardware designs, focusing on a uniform random number generator with a wishbone interface. The study demonstrated using LLMs, specifically Microsoft Bing Chat, to generate Python-based Amaranth HDL scripts. The design was validated through simulations and the Dieharder randomness test suite, showing performance comparable to established random number generators. The methodology facilitated iterative refinement of designs, ensuring correctness and functionality. The final design was successfully deployed on an Icebreaker FPGA, integrating with the Caravel SoC platform. This approach lowers the entry barrier for hardware design, making it more accessible for prototyping domain-specific computing accelerators, with future work aiming to incorporate true random number generators and explore additional HLS tools and languages¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib75" title="">75</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.4.1.1">III-A</span>2 </span>Optimization And Synthesis</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">LLMs are increasingly being employed to enhance the efficiency and quality of hardware designs in optimization and synthesis. These models are leveraged to optimize power, performance, and area (PPA) designs as well as assist in synthesizing HDL code from high-level specifications. By automating and refining these processes, LLMs enable more efficient design implementations and a significant reduction in optimization time.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">A common thread among several studies is the use of LLMs to optimize PPA metrics through innovative frameworks and methodologies. For instance, Thorat et al. introduced the VeriPPA framework to optimize the PPA constraints in Verilog Designs, significantly improving the quality of LLM-generated Verilog code to meet industry standards¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib54" title="">54</a>]</cite>. Similarly, Kaiyan Chang et al. introduced ChipGPT, a framework that optimizes the quality of hardware designs generated by LLMs through a post-LLM search approach. Integrating design space exploration into the workflow improves PPA metrics ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib65" title="">65</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">Chowdhury et al. further advanced this domain by introducing the ABC-RL methodology, which leverages past design data and a retrieval-guided reinforcement learning approach to optimize logic synthesis, achieving up to 24.8% improvement in Quality-of-Result (QoR) and up to 9x runtime reduction compared to state-of-the-art techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib86" title="">86</a>]</cite>. Attention was used in this work to help encode synthesis recipes in a format that allowed the other ML elements of the system to better learn/predict the circuit state resulting from each synthesis optimization step.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.1">Another prominent theme is the use of LLMs for automated HDL/RTL generation and synthesis, often combined with advanced search algorithms to ensure functional correctness and optimality. DeLorenzo et al. proposed a novel approach integrating LLMs with Monte Carlo Tree Search (MCTS) to generate high-quality RTL code ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib59" title="">59</a>]</cite>. Their automated transformer decoding algorithm, guided by MCTS, produces compilable and functionally correct RTL code, significantly improving the area-delay product (ADP) for various hardware modules .
Nazzal et al. introduced the Systolic Array-based Accelerator DataSet (SA-DS), designed to facilitate the application of LLMs in generating and optimizing DNN hardware accelerators¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib67" title="">67</a>]</cite>. The dataset, created using the Gemmini generator, includes a variety of spatial array configurations with accompanying verbal descriptions and Chisel code. The study demonstrates that fine-tuning LLMs with SA-DS significantly improves the quality and efficiency of hardware design generation, highlighting the potential for minimal human intervention.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p5">
<p class="ltx_p" id="S3.SS1.SSS2.p5.1">Beyond PPA optimization and RTL generation, researchers are also exploring the integration of LLMs with electronic design automation (EDA) tools to enhance scalability and usability. Jiang et al. introduced IICPilot, an intelligent IC backend design system utilizing LLMs and a multi-agent framework to automate various backend design processes using open-source EDA tools ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib79" title="">79</a>]</cite>. IICPilot automates script generation, EDA tool invocation, design space exploration of parameters, and dynamic resource allocation through containerization. The framework separates the backend workflow from specific EDA tools via a unified interface and leverages LangChain‚Äôs multi-agent system for tasks such as floorplanning and routing. Experimental validation on platforms like iEDA and OpenROAD demonstrated significant improvements in task completion time and resource utilization, achieving up to 32.75% optimization in PPA metrics. The system‚Äôs scalability and fault tolerance were highlighted, with experiments conducted on a multi-node setup on Alibaba Cloud involving 400 benchmarks from OpenCores. IICPilot enhances IC backend design efficiency, reducing the entry barrier for designers and showing promise for future integration with more EDA tools and platforms.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p6">
<p class="ltx_p" id="S3.SS1.SSS2.p6.1">Ho and Ren further extended the application of LLMs to physical layout optimization, specifically targeting advanced semiconductor nodes at the 2nm technology node¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib82" title="">82</a>]</cite>. Their methodology employs the ReAct prompting technique to integrate human design expertise with LLM reasoning, resulting in up to 19.4% smaller cell area and 23.5% more LVS/DRC clean layouts compared to traditional methods. This work underscores the versatility of LLMs in understanding and optimizing netlist topologies, device clustering constraints, and physical layouts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.4.1.1">III-A</span>3 </span>Verification And Validation</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">Verification and validation are crucial in hardware design to ensure functionality and compliance with specifications. LLMs may automate the generation of test cases, verify the correctness of HDL code, generate hardware assertions, and perform formal verification, thereby identifying design flaws early in the development process. This reduces the time and cost associated with verification.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">A key area of focus has been the development of benchmarking frameworks to systematically evaluate the capabilities of LLMs in hardware design. Lu et al. introduced RTLLM as an open-source benchmark for systematic and quantitative evaluation of the design RTL from the natural language¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib23" title="">23</a>]</cite>, focusing mainly on syntax correctness, functional accuracy, and design quality. They also explored a self-planning prompting technique that they use to improve the performance of the LLMs. Similarly, Mingjie et al. developed VerilogEval, a benchmarking framework for automatic functional correctness testing of Verilog code generations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib24" title="">24</a>]</cite>. They derived the evaluation dataset from HDLBits, an online coding platform for Hardware Engineers. Their study demonstrates that supervised fine-tuning (SFT) with synthetic problem-code pairs can enhance pre-trained LLMs‚Äô Verilog code generation capabilities.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.1">Another prominent theme is the use of LLMs for assertion generation and formal verification, which are essential for ensuring design reliability. Fang et al. proposed AssertLLM, which illustrated how LLMs can generate and evaluate hardware verification assertions, improving the reliability of hardware designs ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib29" title="">29</a>]</cite>. Complementing this, Mali et al. introduced ChiRAAG, a framework that automates the generation of SystemVerilog Assertions(SVAs) from natural language specifications using LLM prompting¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib58" title="">58</a>]</cite>. This research helps to streamline the formal property verification process in hardware design. Further advancing this area, Hassan et al. proposed a formal verification methodology that utilizes OpenAI‚Äôs GPT-4 for automating invariant generation and integrates mutation testing to validate these invariants for hardware designs, demonstrated with the ISCAS-85 C432 27-channel interrupt controller, achieving effective invariant validation and a mutation score of 1 out of 20 mutants¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib60" title="">60</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p4">
<p class="ltx_p" id="S3.SS1.SSS3.p4.1">The automation of testbench generation and functional verification has also been a significant focus. Qui et al.. introduced AutoBench, a systematic and generic framework for generating Verilog testbenches using LLMs for RTL verification. AutoBench combined Python and Verilog codes in a hybrid testbench architecture, enhancing efficiency and effectiveness¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib87" title="">87</a>]</cite>. The framework included comprehensive code generation, scenario checking, standardization, and automatic debugging. AutoEval, an automatic evaluation framework, assessed the generated test benches using multiple metrics, demonstrating higher pass ratios and task completion rates than traditional methods. The self-enhancement system improved test benches through scenario checking and auto-debugging, significantly enhancing coverage and correctness. The study underscored AutoBench‚Äôs impact on automating HDL verification and suggests future research to refine LLM capabilities and integrate with EDA tools for more complex verification tasks.
Similarly, Blocklove et al. evaluated the capability of LLMs like ChatGPT-4 and ChatGPT-3.5 in generating functional Verilog code and corresponding testbenches for hardware design and verification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib70" title="">70</a>]</cite>. The study developed benchmarks to assess LLMs‚Äô performance across various hardware functions and validated the designs through simulation and silicon testing. Results showed that while LLMs can generate functional HDL code, their ability to produce comprehensive testbenches is limited, with ChatGPT-4 outperforming ChatGPT-3.5. The authors emphasize the need for improvements in error understanding, testbench generation, and handling complex designs. The author plans to futher enhance dataset quality, training schemes, and integration with EDA tools to advance the automation of the digital design pipeline.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p5">
<p class="ltx_p" id="S3.SS1.SSS3.p5.1">Beyond general-purpose frameworks, researchers have also investigated the use of LLMs for application-specific hardware design and verification. Vitolo et al. explored using LLMs, specifically ChatGPT-4, to automate the generation and validation of hardware description code for a Recurrent Spiking Neural Network (RSNN) in Verilog¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib68" title="">68</a>]</cite>. The study showcased the successful design of a 3x3x3 RSNN and corresponding test benches through an iterative process facilitated by ChatGPT-4, achieving high accuracy in tasks such as exclusive OR, IRIS flower classification, and MNIST handwritten digit classification. Prototyping on an FPGA and implementing using SkyWater 130 nm technology, the RSNN demonstrated efficient performance metrics. This work underscores the feasibility of using LLMs for complex hardware design, suggesting future research to enhance dataset quality, training schemes, and integration with electronic design automation tools for more sophisticated hardware projects.
Similarly, Thorat et al. discussed the veriRectify process, which uses error diagnostics from simulators to improve the integrity of generations of Verilog code¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib54" title="">54</a>]</cite>.
Finally, the optimization of LLMs for hardware design tasks and their integration with Electronic Design Automation (EDA) tools have emerged as important research directions. Lily Jiaxin Wan et al. explored techniques such as quantization, pruning, and software/hardware co-design to optimize LLMs for efficient inference¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib61" title="">61</a>]</cite>. They applied these optimized models to functional verification in circuit design and introduced the Chrysalis dataset to enhance LLM-based debugging tools. This work aligns with broader efforts to refine LLM capabilities and integrate them into existing EDA workflows, as highlighted by several studies mentioned earlier.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS4.4.1.1">III-A</span>4 </span>Debugging And Maintenance</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">LLMs may play a crucial role in debugging and maintaining hardware designs. They can assist in identifying and fixing syntax errors in HDL code, streamlining the debugging process, and ensuring the correct operation of hardware designs. This facilitates efficient troubleshooting and maintenance.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p2">
<p class="ltx_p" id="S3.SS1.SSS4.p2.1">Researchers have explored the role of LLMs in debugging and maintenance. For instance, YunDa et al. developed an RTLFixer framework that automates syntax corrections in Verilog code using LLMs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib39" title="">39</a>]</cite>. They utilize Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act as autonomous agents in interactively debugging the code with feedback . Similarly, the HDLdebugger by Yoa et al. provided insights into how LLMs can simplify the debugging process for HDL codes, making it easier for hardware engineers to maintain high-quality designs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib62" title="">62</a>]</cite>. They used RAG, which integrates a search engine that retrieves relevant HDL documentation and code instances to enhance the LLM‚Äôs debugging capability.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p3">
<p class="ltx_p" id="S3.SS1.SSS4.p3.1">Another shared focus is on improving the interpretability of error messages and making hardware design tools more accessible to novices. Qiu et al. explored this by using LLMs to generate user-friendly explanations for compile-time synthesis error messages from EDA tools like Quartus Prime and Vivado, demonstrating that approximately 71% of LLM-generated explanations were correct and complete, aiding in educational contexts for novice digital hardware designers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib88" title="">88</a>]</cite>. This aligns with the broader trend of using LLMs to bridge the gap between complex hardware design processes and user comprehension.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p4">
<p class="ltx_p" id="S3.SS1.SSS4.p4.1">In addition to debugging, LLMs have been applied to enhance the reliability of HDL code generation for specific hardware applications. Xiang et al. investigated this by developing a three-phase Pulse Width Modulation (PWM) generator using LLMs. Their study emphasized strategies such as role specification, hierarchical design, and error feedback mechanisms to address syntax errors and high-level semantic challenges in LLM-generated code¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib69" title="">69</a>]</cite>. The successful fabrication and validation of the PWM generator using tools like iVerilog and GTKWave underscore the potential of LLMs in integrated circuit (IC) design. This work also highlights the importance of integrating LLMs with existing EDA tools to improve their practical utility.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p5">
<p class="ltx_p" id="S3.SS1.SSS4.p5.1">Furthermore, LLMs have shown potential in addressing security-relevant hardware bugs. Ahmad et al. investigated the potential of LLMs for automatically repairing security-relevant hardware bugs in Verilog code¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib73" title="">73</a>]</cite>. The study introduced a framework for quantitatively evaluating LLMs‚Äô performance in fixing these bugs and compares their effectiveness against state-of-the-art automated repair tools. Experimental results showed that LLMs, particularly GPT-4, can effectively repair simple hardware security bugs and sometimes outperform existing tools. Detailed prompt engineering significantly enhanced the success rate of LLM-generated repairs. However, LLMs struggled with more complex issues like race conditions and multi-line fixes. The paper highlighted the potential of LLMs in hardware bug repair while acknowledging their limitations and suggests future research should focus on hybrid methods, fine-tuning LLMs for hardware-specific tasks, and improving prompt strategies for more complex bug fixes.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p6">
<p class="ltx_p" id="S3.SS1.SSS4.p6.1">Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.T1" title="TABLE I ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">I</span></a> provides a comprehensive view of different methods and AI models used in digital hardware design from 2022 to 2024. It further classifies these methods into the year they were introduced and their capabilities regarding design assistance, optimization, synthesis, verification, validation, debugging, and maintenance. The last column lists the associated AI models used in each paper, such as GPT, CodeGen, etc. This table highlights the evolution and integration of AI-driven tools in digital hardware design, focusing on methods that involve various levels of assistance in the design and verification process. The presence of diverse models, such as GPT-3.5, GPT-4, and versions of Llama, and some fine-tuned models, such as VeriGen and CodeGen, indicates a rise in the use of powerful AI models over time to support hardware-related tasks.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">LLM-based Analog Hardware Design</span>
</h3>
<figure class="ltx_figure" id="S3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S3.F11.g1" src="x11.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Key stages in the IC design flow showcasing the integration of AI frameworks like ADO-LLM, LLANA, AICircuit, and LayoutCopilot for optimizing schematic, simulation, and physical design processes.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">LLMs have demonstrated transformative potential not only in digital systems but also in analog systems and is now gaining significant traction. These frameworks leverage the contextual understanding, domain knowledge, and generative capabilities of LLMs to address unique challenges in analog circuit design, topology generation, and layout optimization.
A key area of focus is using LLMs to automate and assist in analog circuit design. For instance, Lai et al. introduced AnalogCoder, a training-free LLM agent designed specifically for analog circuit design through Python code generation. AnalogCoder leveraged a feedback-enhanced design flow and a comprehensive circuit tool library to automate the design process¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib89" title="">89</a>]</cite>. It successfully designed 20 out of 24 benchmark circuits, outperforming other LLM-based methods. The feedback mechanisms, which included requirement check, simulation and operating point check, DC sweep check, and function check, significantly improved design success rates. By archiving successful designs as reusable modular sub-circuits, AnalogCoder simplified the creation of complex circuits.
Similarly, Chang et al. introduced LaMAGIC, a pioneering framework using language models to generate optimized analog circuit topologies from user-defined specifications, particularly for power converter applications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib90" title="">90</a>]</cite>. Leveraging supervised fine-tuning, LaMAGIC can efficiently produce high-quality circuit designs in a single pass, significantly reducing design time compared to traditional methods. The framework demonstrated a success rate of up to 96% under strict tolerances and showed superior performance with adjacency-matrix-based formulations for more complex circuits. The model‚Äôs scalability was validated with 6-component circuits, and data augmentation improved generation capabilities. These works underscore the role of LLMs in automating repetitive design tasks and improving design accuracy through iterative feedback and modularization.
Another significant application of LLMs is in optimizing analog circuit design processes. Chen et al. presented LLANA, a framework that leveraged LLMs to enhance bayesian optimization for generating analog design-dependent parameter constraints¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib91" title="">91</a>]</cite>.LLANA reduces simulation requirements while maintaining adaptability across various circuit designs, achieving results comparable to state-of-the-art Bayesian optimization methods with fewer simulations. The framework‚Äôs success is demonstrated in the design of two-stage operational amplifiers, where it achieves better optimization with fewer sampless.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Similarly, Yin et al. introduced ADO-LLM, a framework that combines bayesian optimization with LLMs to enhance analog circuit design efficiency and effectiveness¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib92" title="">92</a>]</cite>. ADO-LLM leverages LLMs‚Äô domain knowledge to generate initial design points and uses Gaussian process-based bayesian optimization to optimize these points iteratively. The framework demonstrated notable improvements in design efficiency and performance on two analog circuits: a two-stage differential amplifier and a hysteresis comparator.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Both frameworks highlight the synergy between LLMs and optimization algorithms, enabling more efficient exploration of design spaces and reducing computational overhead.
LLMs have also been applied to analog layout design, where they assist in optimizing physical layouts and improving verification processes.
Chang et al. introduced LayoutCopilot, an LLM-powered multi-agent collaborative framework designed to enhance the interactive analog layout design process¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib93" title="">93</a>]</cite>. This framework integrates LLMs within a multi-agent system to streamline and optimize design tasks, achieving 6.49% improvement in logical verification, 4.85% in syntactic verification, and 5.89% in overall accuracy using structured instructions. Multi-agent configurations proved more effective, achieving a correctness rate of 96.80% with Claude-3. Post-interactive adjustments, the layout area was reduced to 66% of its original size, with notable gains in performance metrics such as Gain, Phase Margin (PM), and Common-Mode Rejection Ratio (CMRR) tools.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>LLM-based Analog Hardware Design</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.1">
<tr class="ltx_tr" id="S3.T2.1.1" style="background-color:#E3E3E3;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1" style="background-color:#E3E3E3;">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1" style="background-color:#E3E3E3;">Method Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.1" style="background-color:#E3E3E3;">Application</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.1" style="background-color:#E3E3E3;">AI Model</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.2.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.2">LLANA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib91" title="">91</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.3">Design Assistance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.4">GPT3.5</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.2">LaMAGIC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib90" title="">90</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.3">Design Assistance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.4">Language Model</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.4.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.4.2">Analog Coder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib89" title="">89</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.4.3">Design Assistance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.4.4">GPT3.5</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.5.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.5.2">ADO-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib92" title="">92</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.5.3">Optimization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.5.4">GPT-3.5 Turbo</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.6.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.6.2">Layout Copilot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib93" title="">93</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.6.3">Layout Design</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.6.4">GPT-3.5,4,Claude 3</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Figure ¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.F11" title="Figure 11 ‚Ä£ III-B LLM-based Analog Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">11</span></a> illustrates the integration of AI frameworks into the analog IC design flow. The figure primarily highlights how these frameworks optimize various IC design stages. All these AI frameworks are utilized to enhance the efficiency and precision of the hardware design process. This figure showcases the underlying roles of these AI tools in streamlining the analog IC design workflow.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Table ¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.T2" title="TABLE II ‚Ä£ III-B LLM-based Analog Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">II</span></a> summarizes recent works that apply LLMs to analog hardware design. This table lists five significant papers from 2024, highlighting their applications and the AI models they use. The papers: LLANA, LaMAGIC, and Analog Coder focus specifically on design assistance, whereas ADO-LLM and Layout Copilot address optimization and layout design, respectively. In these studies, researchers employ advanced LLMs such as GPT-3.5, GPT-3.5 Turbo, GPT-4, and Claude 3, showcasing their ability to handle complex design processes. These papers emphasize how LLMs increasingly enhance efficiency and accuracy throughout the analog hardware design workflow.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Other Attention-based Hardware Design</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Attention-based models have emerged as a revolutionary approach in the design of hardware, efficiently solving open challenges in areas like floor planning and macro placement¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib94" title="">94</a>]</cite>, logic synthesis, and standard cell design¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib95" title="">95</a>]</cite>. Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.F12" title="Figure 12 ‚Ä£ III-C Other Attention-based Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">12</span></a> provides a structured approach to how attention-based models can be leveraged to these challenges. It starts with identifying the challenges of optimal macro placement, efficient floor planning, and enhancing logic synthesis and standard cell design. These challenges are then tackled by advanced attention-based models, such as GATs, transformers, and diffusion models.
GATs show the unparalleled performance of capturing the dependencies of graph-structured data and could serve well in tasks like logic synthesis or floor planning, wherein the circuit can be visualized as a graph. Transformers capture long-range dependencies and contextual information and are thus applied to tasks such as macro placement and logic synthesis. Often, this is combined with MCTS to refine the generated solution further. It uses diffusion models, known to generate high-quality distributions, for macro placement and standard cell design challenges. Methods like reinforcement learning further enhance these techniques to optimize the solutions. Furthermore, different techniques are employed, such as EAGAT combined with reinforcement learning for floor planning, Transformers with MCTS for logic synthesis, and Diffusion Models for macro placement. This model, when combined, will provide a solid framework for improving efficiency and automation in the hardware design workflow.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">One prominent area of application is floorplanning and macro placement, where optimizing chip area, wire length, and placement legality are critical. Bo Yang et al. introduced an end-to-end reinforcement learning methodology for chip floorplanning that optimizes macro placement and orientation to minimize chip area and wire length, leveraging an edge-aware graph attention network (EAGAT) ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib28" title="">28</a>]</cite>.
Similarly, Lee et al. introduced an approach using diffusion models for macro placement in digital circuit design to overcome RL methods‚Äô limitations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib96" title="">96</a>]</cite>. The study proposed a diffusion model that placed all macros simultaneously, avoiding the sequential limitations associated with RL. A novel neural network architecture combining graph convolutions and multi-headed attention layers was developed for efficient and expressive placement. The authors introduced a method for generating large synthetic datasets for pre-training the diffusion model, facilitating training without relying on proprietary data. Empirical evaluations demonstrated competitive performance on benchmark datasets, showing high legality and low half-perimeter wire length (HPWL) ratios.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In logic synthesis and standard cell design, attention-based models excel because they handle graph-structured data and long-range dependencies. Li et al. introduced the Circuit Transformer. This transformer-based model predicts the next logic gate for end-to-end logic synthesis. It leveraged a novel encoding scheme and integrated Monte-Carlo Tree Search to optimize and improve the generated circuit designs ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib97" title="">97</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="667" id="S3.F12.g1" src="x12.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Overview of attention-based hardware design techniques leveraging models like graph attention networks (GATs), transformers, and diffusion models to address challenges in hardware design.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">In standard cell design, Chia-Tung Ho et al. from Nvidia introduced an innovative transformer-based clustering methodology to improve standard cell design automation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib98" title="">98</a>]</cite>. By training the model with LVS/DRC clean cell layouts and utilizing personalized page rank vectors, the authors addressed challenges in routability for advanced nodes. Their approach significantly enhanced performance, generating 15% more LVS/DRC clean layouts, reducing average cell width by 3.9 %, and total wire length by 3.3%, while achieving a 12.7√ó speedup over previous methods. The study demonstrated the model‚Äôs scalability, achieving 100% LVS/DRC clean layouts for over 1000 single-row cells and reducing cell area by 14.5% compared to industrial standards.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">Scalability and generalizability are key challenges in applying attention-based models to large-scale circuit design. Deng et al. introduced Hop-Wise Graph Attention (HOGA), a novel approach to enhance the scalability and generalizability of GNNs for large-scale circuit representation learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib99" title="">99</a>]</cite>. HOGA precomputes hop-wise features to enable efficient distributed training and adaptively captures high-order structures in circuits. The model demonstrated superior performance, reducing the estimation error for Quality of Results (QoR) prediction by 46.76% and improving reasoning accuracy by 10% over conventional GNNs. Evaluations conducted on the OpenABC-D benchmark and functional reasoning tasks highlighted HOGA‚Äôs ability to handle complex circuit tasks with improved training efficiency and scalability. Despite its higher computational cost, HOGA‚Äôs architecture facilitates massive parallelization, making it suitable for large-scale circuit datasets and presenting a significant advancement in EDA.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">Attention-based models have also been applied to hotspot detection in semiconductor layout design, a critical aspect of design for manufacturability. Zhu et al. introduced a novel single-stage end-to-end hotspot detection framework leveraging multi-task learning and a transformer Encoder to enhance efficiency and accuracy in semiconductor layout design hotspot detections¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib100" title="">100</a>]</cite>. The framework utilizes a single-stage detector with center and corner heads for improved representation learning and a transformer Encoder for global feature aggregation. Experimental results showed the framework achieving a 97.31% detection accuracy and a significant speedup of up to 67.84 times compared to previous methods. It effectively reduced false alarms by 87.6% and 59.5% compared to the TCAD‚Äô19 and DAC‚Äô19 methods, respectively. Evaluations conducted on the ICCAD CAD Contest 2016 Benchmarks demonstrated the framework‚Äôs superior performance. This efficient and robust solution holds promise for advanced design for manufacturability research, with future work suggested to enhance complexity handling and applicability to diverse semiconductor manufacturing processe.
Transferability and automation are increasingly important in chip placement, where reducing runtime and improving placement quality is critical. Yao Lai et al. explored these aspects with ChiPFormer, a transformer-based approach that leveraged offline RL to develop a transferable placement policy, significantly enhancing placement quality and reducing runtime¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib101" title="">101</a>]</cite>. The model demonstrated a 10√ó reduction in runtime and superior half perimeter wire length (HPWL) compared to recent RL-based methods. ChiPFormer effectively transferred learned policies to new chip designs, reducing placement time from hours to minutes, and outperformed existing methods such as GraphPlace, MaskPlace, and DeepPR in both HPWL and runtime across extensive experiments on 32 chip circuits.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">Researcher have also explored the domain of analog and radio-frequency circuit design. Mehradfar et al. introduced AICircuit, a novel multi-level dataset and benchmark designed to advance the application of ML in analog and radio-frequency circuit design¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib102" title="">102</a>]</cite>. The paper highlighted the challenges inherent in analog circuit design, particularly the complexity and time intensity as circuit parameters increased. AICircuit included seven fundamental circuits and two complex wireless transceiver systems, serving as a comprehensive resource for evaluating various ML algorithms. The authors demonstrated the potential of ML models, such as multi-layer perceptrons and transformers, in mapping design specifications to circuit parameters, significantly simplifying the design process. However, they noted that more complex circuits with higher non-linearity remained challenging for ML models, underscoring the need for further optimization.</p>
</div>
<div class="ltx_para" id="S3.SS3.p8">
<p class="ltx_p" id="S3.SS3.p8.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.F12" title="Figure 12 ‚Ä£ III-C Other Attention-based Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">12</span></a> Overview of attention-based hardware design techniques leveraging models like GATs, transformers, and diffusion models to address challenges in floorplanning, logic synthesis, macro placement, and standard cell design.</p>
</div>
<div class="ltx_para" id="S3.SS3.p9">
<p class="ltx_p" id="S3.SS3.p9.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S3.T3" title="TABLE III ‚Ä£ III-C Other Attention-based Hardware Design ‚Ä£ III Attention-Based Hardware design ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes some recent works using attention-based models for hardware design. The table covers six important works from 2021 to 2024, listing the application and AI model used. These papers use the attention-based model for hotspot detection, STA, placement, floorplanning, logic synthesis, standard cell design automation, and representation. The researchers apply different advanced attention-based models, including multiheaded transformers, GAT-based models, decision transformers, and edge-aware graph attention networks. These works show an increasing use of attention mechanisms in hardware design workflows.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Other Attention-based Hardware Design </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tr class="ltx_tr" id="S3.T3.1.1" style="background-color:#E3E3E3;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1" style="background-color:#E3E3E3;">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.2.1" style="background-color:#E3E3E3;">Method Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.1" style="background-color:#E3E3E3;">Application</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.4.1" style="background-color:#E3E3E3;">AI Model</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.1">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib100" title="">100</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.3">Hotspot Detection</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.4">Multi headed transformer</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.3.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.2">Deep EdgeGAT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib25" title="">25</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.3">STA Analysis</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.4">GAT-based model</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.4.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.2">ChiPFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib101" title="">101</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.3">Placement</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.4">Decision Transformer</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.5.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib28" title="">28</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.3">Floorplanning</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.4">Edge-aware graph attention network</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.6.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.6.2">Circuit Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib97" title="">97</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.6.3">Logic Synthesis</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.6.4">Transformer-based model</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.7.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.7.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib98" title="">98</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.7.3">Standard Cell Design Automation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.7.4">Transformer model</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.8.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.8.2">HOGA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib99" title="">99</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.8.3">Representation</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.8.4">Attention-based model</td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Attention-Based hardware security</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">LLM-based Hardware Security</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">This section presents the role of LLMs in enhancing hardware security on various tiers of applications such as vulnerability detection, threat analysis, security verification, and logic obfuscation. We also explore frameworks on automated debugging, security assertion generation, and thwarting side-channel attacks. A common theme running across these works is to address the challenge of scalability, accuracy, and efficiency in hardware security using LLMs.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.F13" title="Figure 13 ‚Ä£ IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">13</span></a> highlights key security vulnerabilities and challenges in hardware design, including fault injection attacks, SoC security, logic obfuscation, and side-channel attacks. It illustrates the potential of LLM/attention models such as GPT-3.5, Llama3-70B, GNNs, and HS-BERT in enhancing security by addressing detection accuracy, scalability, debugging efforts, and dataset limitations, enabling effective threat detection, logic obfuscation, side-channel mitigation, and SoC property generation.</p>
</div>
<figure class="ltx_figure" id="S4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="600" id="S4.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Overview of security vulnerabilities, challenges, and the role of LLMs and Attention-based models in detection, mitigation, and debugging for hardware security.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.4.1.1">IV-A</span>1 </span>Vulnerability Detection And Threat Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">A significant focus of recent research has been on leveraging LLMs for vulnerability detection and threat analysis in hardware systems.
Lin et al. presented the HW-V2W-Map framework, an NLP-based system focused on IoT hardware vulnerabilities¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib103" title="">103</a>]</cite>. This framework uses ontology-driven storytelling to analyze and mitigate vulnerabilities, predict future exposures, and provide mitigation suggestions using GPT models. The framework offers an interactive GUI for identifying mitigation strategies and determining exploit and impact scores with the Common Vulnerability Scoring System. Similarly, Saha et al. presented Vul-FSM, a database containing 10,000 vulnerable finite state machine (FSM) designs featuring 16 specific security vulnerabilities¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib104" title="">104</a>]</cite>. The paper introduces SecRT-LLM, leveraging LLMs to create extensive hardware security datasets efficiently. Results demonstrate the framework‚Äôs effectiveness: initial attempts achieve an average pass rate of 81.98% for vulnerability insertion and 97.37% for detection, improving to 80.30% and 99.07%, respectively, within five attempts. This highlights LLMs‚Äô proficiency in enhancing ML-based methods for hardware security benchmarking and mitigation strategies.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">Another critical area of research is the automation of security verification and debugging processes. Akyash et al. introduced Self-HWDebug, a framework designed to enhance the scalability and efficiency of LLMs in automating the mitigation of security vulnerabilities in hardware designs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib105" title="">105</a>]</cite>. The framework leveraged LLMs to autonomously generated detailed debugging instructions, addressing the challenge of manually crafting instructions that demands significant time and expertise. Self-HWDebug prompts LLMs to create targeted mitigation instructions by utilizing pre-identified hardware bugs and resolutions, extending solutions across similar vulnerabilities in different designs. Initial testing shows the framework‚Äôs efficacy in reducing human intervention and improving debugging quality.
Complementing this work, Saha et al. explored the integration of LLMs, particularly GPTs, into the security verification process of SoC designs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib106" title="">106</a>]</cite>. Current security solutions struggle with scalability and adaptability, making them inadequate for modern SoCs. The research aims to create a more efficient and comprehensive security verification methodology by leveraging LLMs. The paper analyzes existing works, provides case studies and experiments, and offers guidelines for using LLMs in SoC security. It highlights the potential benefits and challenges of this innovative approach.
Formal verification has emerged as a key technique for identifying and mitigating Common Weakness Enumerations (CWEs) in LLM-generated hardware designs. Gadde et al. introduced ReFormAI, a dataset of 60,000 SystemVerilog designs generated by various LLMs, focusing on identifying and addressing CWEs through formal verification, demonstrating that approximately 60% of LLM-generated designs are prone to CWEs, and highlighting the superior reliability of formal verification over traditional testing methods. Gadde et al. investigated the prevalence of CWEs in SystemVerilog hardware designs generated by LLMs and evaluated the effectiveness of formal verification in identifying these vulnerabilities¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib107" title="">107</a>]</cite>. They introduced the ReFormAI dataset, comprising 60,000 SystemVerilog designs generated by four different LLMs targeting various CWEs. The study revealed a high vulnerability rate, with approximately 60% of the generated designs containing CWEs. Formal verification techniques successfully identified and categorized these vulnerabilities. Among the evaluated LLMs, GPT-3.5-Turbo performed best but still produced a significant number of vulnerable designs. The study underscores the need for better training datasets and fine-tuning techniques to reduce CWEs in LLM-generated hardware designs.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">Building on this, Fu et al. presented Hardware Phi-1.5B, an LLM tailored explicitly for the hardware domain, addressing challenges in hardware design and security verification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib74" title="">74</a>]</cite>. The model leveraged a specialized, tiered dataset focused on hardware-specific content, demonstrating advancements in predicting the next tokens and handling complex hardware design tasks. Experimental results validated the model‚Äôs proficiency, particularly on the Common Weakness Enumeration (CWE) dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">The automation of hardware security assertion generation is another promising application of LLMs. Kande et al. investigated the potential of LLMs in automating the generation of hardware security assertions ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib108" title="">108</a>]</cite>. By introducing a new evaluation framework and benchmark suite, the study explored the effectiveness of LLMs in generating correct security assertions for hardware designs. OpenAI‚Äôs Codex showed a modest success rate, generating correct assertions 4.53% of the time, with detailed prompts improving performance. The study used Siemens Modelsim for simulation and validation, demonstrating the framework‚Äôs scalability to other LLMs. Despite challenges, such as the need for detailed prompts and context, the results provide a proof-of-concept for LLM-assisted assertion generation, suggesting future work on fine-tuning models, improving datasets, and exploring quick evaluation methods to enhance accuracy and effectiveness in hardware security applications.
Recent efforts have also explored the use of LLMs for automated Trojan detection and mitigation in analog designs. Chaudhuri et al. developed SPICED+, a software-based framework that leverages LLM-enhanced analysis of SPICE netlists and simulation logs to iteratively detect, localize, and remove analog hardware Trojans¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib109" title="">109</a>]</cite>. The framework combines rule-based prompting, few-shot learning, and voltage/current deviation analysis to classify Trojan-impacted nodes and applies an iterative correction loop guided by a confidence scoring mechanism. By integrating with HSPICE for netlist resimulation and validation, SPICED+ achieves an average Trojan coverage of 93.3%, a mitigation rate of 94.6%, and maintains a low false positive rate of 1.4% , outperforming prior ML-based tools like DAWN. Its model-agnostic LLM workflow enables generalization to new circuits without retraining, though large-scale designs such as SAR-ADCs may require modular decomposition for efficient context processing.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.4.1.1">IV-A</span>2 </span>SoC Security</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">LLM-based tools have demonstrated significant potential in automating and enhancing the security verification process for SoC designs. These tools address the limitations of traditional methods by leveraging NLP and LLMs to extract security properties, identify vulnerabilities, and generate comprehensive security policies.For instance, Meng et al introduced NSPG, an innovative NLP-based tool designed to automate the generation of security properties from SoC documentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib110" title="">110</a>]</cite>. The tool leverages the HS-BERT model, a hardware security-specific language model trained on sentences from RISC-V, OpenRISC, MIPS, OpenSPARC, and OpenTitan SoC documentation. In evaluations on five untrained OpenTitan hardware IP documents, NSPG successfully extracted 326 security properties from 1723 sentences, aiding in the identification of eight security bugs presented in the Hack@DAC 2022 competition. This approach demonstrates superior performance compared to traditional methods and popular tools like ChatGPT, highlighting its potential to streamline the security validation process and improve the robustness of SoC designs. The success of NSPG suggests a promising future for NLP applications in hardware security and SoC verification. Similarly, Tarek et al. introduce SoCureLLM, an LLM-based framework designed to improve hardware security verification for complex SoC designs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib111" title="">111</a>]</cite>. SoCureLLM addresses the limitations of traditional methods by effectively identifying security vulnerabilities and generating a comprehensive security policy database. In evaluations, it detected 76.47% of security bugs in three vulnerable RISC-V SoCs, surpassing existing methods. Additionally, SoCureLLM formulated 84 new security policies for large-scale RISC-V SoC designs, enhancing the security policy database. This framework demonstrates significant improvements in adaptability, scalability, Trojan insertion analysis, and efficiency for SoC security verification.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.4.1.1">IV-A</span>3 </span>Trojan Insertion</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Research in hardware security has increasingly leveraged LLMs for generating, inserting, and detecting hardware trojans. These studies demonstrate the dual-use nature of LLMs, addressing challenges such as dataset scarcity, context length limitations, and the need for diverse HT examples in hardware design.
Kokolakis et al. investigate the potential of LLMs in offensive hardware security, particularly for inserting hardware trojans into complex hardware designs like CPUs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib112" title="">112</a>]</cite>. They test how well LLMs can correlate high-level security concepts with specific hardware modules to overcome context length limitations. By analyzing reduced code bases and identifying modules with security-related features, they simplify the overall analysis. They demonstrate the LLM‚Äôs capability to modify code and insert HT functionalities. The approach is validated by creating and testing an HT in a RISC-V micro-architecture on an FPGA board, showcasing the LLM‚Äôs ability to aid in realistic HT attacks.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">Complementing this work, Hayashi et al. developed a comprehensive dataset consisting of 290 Verilog examples generated using ChatGPT-4. This dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib113" title="">113</a>]</cite>, derived from 29 golden models and the TrustHub taxonomy, significantly enhances the limited existing datasets for hardware trojans, particularly for RISC-V and Web3 applications. It addresses the gap in research on Web3 hardware trojans, including modules like hardware wallets. This new dataset is intended to facilitate future research on defense mechanisms against hardware trojans in RISC-V systems, hardware wallets, and hardware Proof of Work (PoW) miners.
Building on these advancements, Bhandari et al. introduced SENTAUR, a framework leveraging to generate, detect, and assess hardware Trojans in RTL designs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib114" title="">114</a>]</cite>, addressing security challenges in the IC supply chain. SENTAUR utilizes GPT-4 to create synthesizable RTL with embedded HTs, validated on the Xilinx FPGA platform. It explores various HT triggers and effects, demonstrating effectiveness in real-world applications like AES-T800 and dual-port RAM. A comparative analysis showed SENTAUR‚Äôs superiority over existing tools in generating diverse and effective hypotheses. Experimental results highlighted SENTAUR‚Äôs efficiency and robustness, demonstrating it to be a versatile toolchain for both attackers and defenders. Future work may extend its capabilities to post-synthesized netlist functionalities and various hardware platforms, further enhancing its adaptability and effectiveness.Expanding LLM applications into the analog domain, Chaudhuri et al. proposed LATENT ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib115" title="">115</a>]</cite>, a feedback-guided agentic framework that autonomously inserts stealthy analog hardware Trojans into A/MS netlists using a Thought-Action-Observation loop and SPICE-based simulation feedback. LATENT achieves low activation ranges (15.7%) and area overhead (7.4%) while inducing significant output degradation (11.3%), outperforming fixed-pattern Trojans like A2 and DELTA. However, extending LATENT to support AC and small-signal behavior could improve its applicability to a broader range of analog threat scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS4.4.1.1">IV-A</span>4 </span>Logic Obfuscation</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">Researchers have explored leveraging LLMs for logic obfuscation, with one notable work by Latibari et al.introducing Obfus-chat, an innovative framework utilizing GPT models to automate the obfuscation of hardware IPs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib116" title="">116</a>]</cite>. This system takes hardware design netlists and key sizes as inputs, producing obfuscated code to enhance security. The framework‚Äôs effectiveness is assessed using the Trust-Hub Obfuscation Benchmark, incorporating SAT attacks and functional verification to ensure security and design integrity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS5.4.1.1">IV-A</span>5 </span>Side-Channel Attack</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">Side-channel attacks (SCAs) remain a critical threat to hardware security, with power and cache-based attacks being among the most prominent.
Researchers have leveraged LLMs and graph-based techniques to address side-channel vulnerabilities, particularly power and cache-based attacks. These approaches automate vulnerability detection, generate protective measures, and improve the interpretability of security analyzes, enabling early-stage vulnerability identification and reducing the need for costly post-silicon fixes.
A significant focus has been on power side channel (PSC) attacks, which exploit power consumption patterns to leak sensitive information. Srivastava et al. addressed this challenge with SCAR, a pre-silicon PSC analysis framework that utilizes GNNs to convert designs into control-data flow graphs for detecting modules susceptible to side-channel leakage¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib117" title="">117</a>]</cite>. The framework includes a deep-learning-based explainer for providing human-accessible explanations of detection decisions and a fortification component that uses LLMs to generate and insert protective design code at vulnerable points. SCAR achieves significant figures of 94.49% localization accuracy, 100% precision, and 90.48% recall on various encryption algorithms, including AES, RSA, PRESENT, Saber, and CRYSTALS-Kyber. This significantly improves design robustness and efficiency by enabling early vulnerability detection and reducing the need for costly post-silicon fixes. Additionally, SCAR reduces features for GNN model training by 57% while maintaining comparable accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.1">Cache-based side-channel attacks have also been a focal point, with researchers leveraging LLMs to model and evaluate secure cache architectures. He et al. propossed a novel probabilistic information flow graph to model interactions between victim and attacker programs and the cache architecture¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib118" title="">118</a>]</cite>. It introduces the Probability of Attack Success (PAS) metric to quantitatively assess a cache‚Äôs resilience against various cache side-channel attacks. This model and metric are applied to evaluate nine different cache architectures across four classes of such attacks. LLMs are leveraged to automate the analysis and generation of potential attack scenarios. This approach enables verification and comparison of different secure cache architectures‚Äô resilience to side-channel attacks without requiring simulation or hardware implementation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p3">
<p class="ltx_p" id="S4.SS1.SSS5.p3.1">Table ¬†<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#S4.T4" title="TABLE IV ‚Ä£ IV-A5 Side-Channel Attack ‚Ä£ IV-A LLM-based Hardware Security ‚Ä£ IV Attention-Based hardware security ‚Ä£ Hardware Design and Security Needs Attention: From Survey to Path Forward"><span class="ltx_text ltx_ref_tag">IV</span></a> summarizes the latest contributions that leverage LLMs for secure hardware design. This table lists 13 influential works from 2023 and 2024, showing how researchers apply LLMs to various security applications, including vulnerability detection, SoC security, Trojan insertion, logic obfuscation, and side-channel analysis. The works utilize advanced AI models such as GPT-3.5 Turbo, GPT-4, HS-BERT, code-davinci-002, Llama-70B, and GNNs. Each paper showcases the ability of these models to tackle specific security challenges, enhancing hardware resilience and protecting against emerging threats. These contributions highlight how researchers increasingly rely on LLMs and attention-based methods to innovate and strengthen hardware security practices.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>LLM-based secure hardware design </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.1">
<tr class="ltx_tr" id="S4.T4.1.1" style="background-color:#E3E3E3;">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1" style="background-color:#E3E3E3;">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.2">
<span class="ltx_inline-block" id="S4.T4.1.1.2.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S4.T4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.1.1.1">Method</span></span>
<span class="ltx_p" id="S4.T4.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.1.2.1">Name</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3">
<span class="ltx_inline-block" id="S4.T4.1.1.3.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S4.T4.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.1.1.1">Vulnerability </span></span>
<span class="ltx_p" id="S4.T4.1.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.1.2.1">Detection</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.4">
<span class="ltx_inline-block" id="S4.T4.1.1.4.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S4.T4.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.1.1.1">SoC</span></span>
<span class="ltx_p" id="S4.T4.1.1.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.1.2.1">Security</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5">
<span class="ltx_inline-block" id="S4.T4.1.1.5.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S4.T4.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.1.1.1">Trojan</span></span>
<span class="ltx_p" id="S4.T4.1.1.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.1.2.1">Insertion</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.6">
<span class="ltx_inline-block" id="S4.T4.1.1.6.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S4.T4.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.1.1.1">Logic</span></span>
<span class="ltx_p" id="S4.T4.1.1.6.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.1.2.1">Obfuscation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.7">
<span class="ltx_inline-block" id="S4.T4.1.1.7.1" style="background-color:#E3E3E3;">
<span class="ltx_p" id="S4.T4.1.1.7.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.1.1.1">Side</span></span>
<span class="ltx_p" id="S4.T4.1.1.7.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.1.2.1">Channel</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.1" style="background-color:#E3E3E3;">AI Model</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.2.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2">Lin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib103" title="">103</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.2.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.2.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.2.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.2.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.8">GPT-3.5-Turbo</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.3.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.2">Thakur et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib56" title="">56</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.3.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.3.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.3.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.8">GPT-3.5-Turbo</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.4.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.2">Saha et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib106" title="">106</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.4.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.4.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.4.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.4.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.8">GPT-3 and GPT-4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.5.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.2">Meng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib110" title="">110</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.5.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.5.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.5.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.5.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.8">HS-BERT</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.6.1">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.6.2">kande et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib108" title="">108</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.6.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.6.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.6.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.6.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.6.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.6.8">code-davinci-002</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.7.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.7.2">Saha et al.(Vul-FSM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib104" title="">104</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.7.3">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.7.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.7.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.7.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.7.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.7.8">GPT-3.5-Turbo</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.8.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.8.2">Gadde et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib107" title="">107</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.8.3">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.8.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.8.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.8.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.8.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.8.8">GPT-3.5-Turbo</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.9.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.9.2">Akyash et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib105" title="">105</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.9.3">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.9.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.9.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.9.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.9.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.9.8">Llama3-70B</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.10.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.10.2">Tarek et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib111" title="">111</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.10.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.10.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.10.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.10.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.10.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.10.8">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.11.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.11.2">kardakis et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib112" title="">112</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.11.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.11.4">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.11.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.11.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.11.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.11.8">GPT-3.5/4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.12.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.12.2">Hayashi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib113" title="">113</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.12.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.12.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.12.5">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.12.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.12.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.12.8">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.13.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.13.2">Latibari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib116" title="">116</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.13.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.13.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.13.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.13.6">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.13.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.13.8">GPT 3.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.14.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.14.2">Bhandari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib114" title="">114</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.14.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.14.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.14.5">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.14.6">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.14.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.14.8">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.15.1">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.15.2">Srivastava et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib117" title="">117</a>]</cite>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.15.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.15.4">‚úì</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.15.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.15.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.15.7">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.15.8">GNN</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.16">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.16.1">2025</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.2">Chaudhuri et al.(LATENT)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib115" title="">115</a>]</cite>
</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.3"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.4"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.5">‚úì</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.6"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.7"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.16.8">GPT-4o-mini</td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Other Attention-based Hardware Security </span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Attention-based models have played a pivotal role in transforming hardware security, enabling more advanced threat detection and mitigation strategies.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">A common theme across recent works is the use of graph-based representations. For instance, Chen et al. proposed TrojanFormer, a resource-efficient method for hardware Trojan detection using a graph transformer network. It converts HDL designs into graph structures to enhance accuracy and efficiency and employs a NodeFormer-based network with linear-complexity message passing and edge-regularization loss. TrojanFormer was evaluated on a Trust-Hub dataset with over 100,000 nodes, achieving a 97.66% F1 score on small and medium ICs. For large ICs, it showed a 4% accuracy improvement and an 18% reduction in computational overhead. Key limitations include difficulty capturing long-range dependencies in large ICs and reduced performance on circuits with sparse or incomplete graph topologies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib119" title="">119</a>]</cite>.
Another key direction is the integration of transformer models with graph-based pre-processing layers.
For instance, Li e t al. proposed a transformer-based method that usages both transformer and Graph Convolutional Network as a pre-processing layer for detecting and localizing HTs at the RTL level and tested their architecture on the TrustHub dataset. They use RTL features to detect and localize HT attacks ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib120" title="">120</a>]</cite>.
In another endeavor, Zhang et al. proposed a novel bit-level Hardware Trojan localization approach, namely B-HTRecognizer, which leveraged Graph Attention Networks. It converted the RTL Verilog code into bit-level edge-featured Data Flow Graphs to capture multi-dimensional feature representation, thus enhancing the precision of HT detection and localization. They presented TrustHub IMEex, a scaled open-source dataset for ML training. B-HTRecognizer achieved 84% precision, 93% recall for non-cross-designs, and 77% recall for cross-design tests on RISC-V, proving effective for various designs. It characterized the progress in automation and fine-grained localization for pre-silicon hardware security ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib121" title="">121</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Finally, we note that few works have considered evaluating the quality of LLM-generated hardware designs from the security perspective, with recent work by Afsharmazayejani et al. providing an initial exploration of how such benchmarking could be undertaken ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.08854v2#bib.bib122" title="">122</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Future Direction</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The application of attention-based models, especially LLMs, in hardware design and security represents a promising frontier with immense innovation potential. One significant research direction is the development of advanced code generation capabilities. While fine-tuning LLMs on domain-specific datasets for HDLs, such as Verilog, is already underway, further improvements can be achieved by leveraging larger and more diverse datasets. These models can improve code accuracy, functional correctness, and performance optimization and streamline the design process. Additionally, integrating multi-modal LLMs capable of processing textual descriptions, visual schematics, and design gate-level netlists could revolutionize hardware design workflows by providing a holistic and comprehensive approach to hardware representation. Automating the generation of detailed, error-free architectural specifications using attention-based models could reduce early design errors and significantly mitigate downstream challenges. In HLS, LLMs can be crucial in simplifying compatibility fixes and adaptive debugging, easing the transformation from C/C++ to HDL. This efficiency can make the overall process smoother and more reliable. In the domain of hardware security, LLMs show great promise in improving the safety and robustness of hardware systems. Secure HDL code generation, powered by domain-trained models, can address risks associated with hardware Trojans and IP piracy.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Furthermore, these models can identify and rectify vulnerabilities in existing designs, ensuring compliance with high-assurance security standards. Automating formal verification tasks, such as generating SystemVerilog assertions and defining security properties, could accelerate the verification and validation phases. Attention-based models can also aid in detecting malicious modifications, such as hardware Trojans, and attack the design by inserting Trojans. LLMs can also be instrumental in automating EDA workflows. Potentially, autonomous agents powered by LLMs could oversee the entire design, testing, and debugging lifecycle with minimal human intervention. Tools like ChatEDA could evolve to encompass the full chip design process, from RTL development to manufacturing, ensuring seamless integration and scalability. Collaboration among multi-agent LLM systems could solve complex problems in hardware design and security, fostering greater intelligence and collaboration across the design ecosystem.
Finally, advancing LLM capabilities in hardware design and security requires improvements in both data availability and model efficiency. Building comprehensive benchmark datasets that cover diversified security scenarios, high-performance designs, and real HDL issues will be critical for effective model training and evaluation. Additionally, lightweight and domain-specific attention-based models could be developed to address resource-constrained environments, such as edge devices or localized hardware labs. Collectively, these advancements have the potential to transform hardware design and security, creating systems that are more area-sparing, energy-efficient, and trustworthy. They can be used to educate the students, familiarize them with hardware design and security, and address workforce development.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Integrating attention-based models, especially LLMs, into hardware design and security is at a transformative stage, offering groundbreaking automation, efficiency, and robustness capabilities. This paper has presented a detailed investigation into how these models are applied to automate HDL generation, optimize chip design workflows, and enhance security mechanisms against threats such as hardware Trojans and side-channel attacks. The state-of-the-art survey shows the tremendous improvement made so far and the challenges ahead, such as data scarcity and the limitation of general-purpose models applied to hardware-specific tasks. Although the latest achievements in attention mechanisms and fine-tuned models have provided significant possibilities, challenges persist, including the need for domain-specific datasets and lightweight models.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.¬†Yang, H.¬†Liu, Y.¬†Zhao, and T.¬†Deng, ‚ÄúA new design approach of hardware implementation through natural language entry,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IET Collaborative Intelligent Manufacturing</em>, vol.¬†5, no.¬†4, p. e12087, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M.¬†Tomlinson, J.¬†Li, and A.¬†Andreou, ‚ÄúDesigning silicon brains using llm: Leveraging chatgpt for automated description of a spiking neuron array,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez, L.¬†Kaiser, and I.¬†Polosukhin, ‚ÄúAttention is all you need,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B.¬†Saber¬†Latibari, S.¬†Salehi, H.¬†Homayoun, and A.¬†Sasan, ‚ÄúIret: Incremental resolution enhancing transformer,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Great Lakes Symposium on VLSI 2024</em>, ser. GLSVLSI ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024, p. 620‚Äì625. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3649476.3660380</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S.¬†Kardakis, I.¬†Perikos, F.¬†Grivokostopoulou, and I.¬†Hatzilygeroudis, ‚ÄúExamining attention mechanisms in deep learning models for sentiment analysis,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Applied Sciences</em>, vol.¬†11, no.¬†9, 2021. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.mdpi.com/2076-3417/11/9/3883</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B.¬†S. Latibari, N.¬†Nazari, M.¬†A. Chowdhury, K.¬†I. Gubbi, C.¬†Fang, S.¬†Ghimire, E.¬†Hosseini, H.¬†Sayadi, H.¬†Homayoun, S.¬†Salehi <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et¬†al.</em>, ‚ÄúTransformers: A security perspective,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">IEEE Access</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
OpenAI, ‚ÄúGpt-4 technical report,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J.¬†Blocklove, S.¬†Garg, R.¬†Karri, and H.¬†Pearce, ‚ÄúChip-chat: Challenges and opportunities in conversational hardware design,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD)</em>, 2023, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.¬†Thakur, J.¬†Blocklove, H.¬†Pearce, B.¬†Tan, S.¬†Garg, and R.¬†Karri, ‚ÄúAutochip: Automating hdl generation using llm feedback,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2311.04887</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B.¬†Yu, ‚ÄúMachine learning in eda: When and how,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2023 ACM/IEEE 5th Workshop on Machine Learning for CAD (MLCAD)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R.¬†Zhong, X.¬†Du, S.¬†Kai, Z.¬†Tang, S.¬†Xu, H.-L. Zhen, J.¬†Hao, Q.¬†Xu, M.¬†Yuan, and J.¬†Yan, ‚ÄúLlm4eda: Emerging progress in large language models for electronic design automation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2401.12224</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L.¬†Chen, ‚ÄúThe dawn of ai-native eda: Promises and challenges of large circuit models,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2403.07257</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A.¬†Sharma, T.-D. Ene, K.¬†Kunal, M.¬†Liu, Z.¬†Hasan, and H.¬†Ren, ‚ÄúAssessing economic viability: A comparative analysis of total cost of ownership for domain-adapted large language models versus state-of-the-art counterparts in chip design coding assistance,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2024 IEEE LLM Aided Design Workshop (LAD)</em>, 2024, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
N.¬†Wu, Y.¬†Li, H.¬†Yang, H.¬†Chen, S.¬†Dai, C.¬†Hao, C.¬†Yu, and Y.¬†Xie, ‚ÄúSurvey of machine learning for software-assisted hardware design verification: Past, present, and prospect,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ACM Trans. Des. Autom. Electron. Syst.</em>, vol.¬†29, no.¬†4, Jun. 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3661308</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J.¬†Yang, ‚ÄúHarnessing the power of llms in practice: A survey on chatgpt and beyond,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M.¬†Akyash, ‚ÄúEvolutionary large language models for hardware security: A comparative survey,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H.¬†Pearce and B.¬†Tan, ‚ÄúLarge language models for hardware security (invited, short paper),‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)</em>, 2024, pp. 420‚Äì423.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D.¬†Bahdanau, K.¬†Cho, and Y.¬†Bengio, ‚ÄúNeural machine translation by jointly learning to align and translate,‚Äù Jan. 2015, 3rd International Conference on Learning Representations, ICLR 2015 ; Conference date: 07-05-2015 Through 09-05-2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P.¬†Veliƒçkoviƒá, G.¬†Cucurull, A.¬†Casanova, A.¬†Romero, P.¬†Li√≤, and Y.¬†Bengio, ‚ÄúGraph attention networks,‚Äù 2018. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/1710.10903</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.¬†Devlin, M.-W. Chang, K.¬†Lee, and K.¬†Toutanova, ‚ÄúBERT: Pre-training of deep bidirectional transformers for language understanding,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, J.¬†Burstein, C.¬†Doran, and T.¬†Solorio, Eds.¬†¬†¬†Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171‚Äì4186. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/N19-1423/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M.¬†Tehranipoor, K.¬†Zamiri¬†Azar, N.¬†Asadizanjani, F.¬†Rahman, H.¬†Mardani¬†Kamali, and F.¬†Farahmandi, ‚ÄúLarge language models for soc security,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Hardware Security: A Look into the Future</em>.¬†¬†¬†Springer, 2024, pp. 255‚Äì299.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
‚ÄúAi is reshaping chip design. but where will it ends?‚Äù <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www-forbes-com.cdn.ampproject.org/c/s/www.forbes.com/sites/karlfreund/2023/12/19/ai-is-reshaping-chip-design-but-where-will-it-end/amp/</span>, accessed: 2024-01-22.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y.¬†Lu, S.¬†Liu, Q.¬†Zhang, and Z.¬†Xie, ‚ÄúRtllm: An open-source benchmark for design rtl generation with large language model,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 29th Asia and South Pacific Design Automation Conference</em>, ser. ASPDAC ‚Äô24.¬†¬†¬†IEEE Press, 2024, p. 722‚Äì727. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1109/ASP-DAC58780.2024.10473904</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M.¬†Liu, N.¬†Pinckney, B.¬†Khailany, and H.¬†Ren, ‚ÄúVerilogeval: Evaluating large language models for verilog code generation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y.¬†Ye, T.¬†Chen, Y.¬†Gao, H.¬†Yan, B.¬†Yu, and L.¬†Shi, ‚ÄúGraph-learning-driven path-based timing analysis results predictor from graph-based timing analysis,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 28th Asia and South Pacific Design Automation Conference</em>, 2023, pp. 547‚Äì552.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T.¬†Chowdhury, A.¬†Vakil, B.¬†Saber¬†Latibari, S.¬†Aresh Beheshti¬†Shirazi, A.¬†Mirzaeian, X.¬†Guo, S.¬†Manoj¬†PD, H.¬†Homayoun, I.¬†Savidis, L.¬†Zhao <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et¬†al.</em>, ‚ÄúRapta: A hierarchical representation learning solution for real-time prediction of path-based static timing analysis,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">Proceedings of the Great Lakes Symposium on VLSI 2022</em>, 2022, pp. 493‚Äì500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S.¬†A. Beheshti-Shirazi, N.¬†Nazari, K.¬†I. Gubbi, B.¬†S. Latibari, S.¬†Rafatirad, H.¬†Homayoun, A.¬†Sasan, and P.¬†S. Manoj, ‚ÄúAdvanced reinforcement learning solution for clock skew engineering: Modified q-table update technique for peak current and ir drop minimization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE Access</em>, vol.¬†11, pp. 87‚Äâ869‚Äì87‚Äâ886, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
B.¬†Yang, ‚ÄúFloorplanning with edge-aware graph attention network and hindsight experience replay,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">ACM Transactions on Design Automation of Electronic Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
W.¬†Fang, M.¬†Li, M.¬†Li, Z.¬†Yan, S.¬†Liu, H.¬†Zhang, and Z.¬†Xie, ‚ÄúAssertllm: Generating and evaluating hardware verification assertions from design specifications via multi-llms,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K.¬†Chang, Y.¬†Wang, H.¬†Ren, M.¬†Wang, S.¬†Liang, Y.¬†Han, H.¬†Li, and X.¬†Li, ‚ÄúChipgpt: How far are we from natural language hardware design,‚Äù 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2305.14019</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C.¬†Lv, Z.¬†Wei, W.¬†Qian, J.¬†Ye, C.¬†Feng, and Z.¬†He, ‚ÄúGpt-ls: Generative pre-trained transformer with offline reinforcement learning for logic synthesis,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">2023 IEEE 41st International Conference on Computer Design (ICCD)</em>, 2023, pp. 320‚Äì326.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Z.¬†Guo, M.¬†Liu, J.¬†Gu, S.¬†Zhang, D.¬†Z. Pan, and Y.¬†Lin, ‚ÄúA timing engine inspired graph neural network model for pre-routing slack prediction,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 59th ACM/IEEE Design Automation Conference</em>, ser. DAC ‚Äô22.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2022, p. 1207‚Äì1212. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3489517.3530597</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.¬†Maynard and A.¬†Rezaei, ‚ÄúReconfigurable run-time hardware trojan mitigation for logic-locked circuits,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">2024 IEEE 17th Dallas Circuits and Systems Conference (DCAS)</em>.¬†¬†¬†IEEE, 2024, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R.¬†Vishwakarma and A.¬†Rezaei, ‚ÄúUncertainty-aware hardware trojan detection using multimodal deep learning,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">2024 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</em>.¬†¬†¬†IEEE, 2024, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y.¬†Aghamohammadi and A.¬†Rezaei, ‚ÄúLipstick: Corruptibility-aware and explainable graph neural network-based oracle-less attack on logic locking,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)</em>.¬†¬†¬†IEEE, 2024, pp. 606‚Äì611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
R.¬†Vishwakarma and A.¬†Rezaei, ‚ÄúRisk-aware and explainable framework for ensuring guaranteed coverage in evolving hardware trojan detection,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)</em>.¬†¬†¬†IEEE, 2023, pp. 01‚Äì09.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
K.¬†I. Gubbi, B.¬†S. Latibari, M.¬†A. Chowdhury, A.¬†Jalilzadeh, E.¬†Y. Hamedani, S.¬†Rafatirad, A.¬†Sasan, H.¬†Homayoun, and S.¬†Salehi, ‚ÄúOptimized and automated secure ic design flow: A defense-in-depth approach,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">IEEE Transactions on Circuits and Systems I: Regular Papers</em>, vol.¬†71, no.¬†5, pp. 2031‚Äì2044, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S.¬†Liu, W.¬†Fang, Y.¬†Lu, Q.¬†Zhang, H.¬†Zhang, and Z.¬†Xie, ‚ÄúRtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2024 IEEE LLM Aided Design Workshop (LAD)</em>, 2024, pp. 1‚Äì5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y.¬†Tsai, M.¬†Liu, and H.¬†Ren, ‚ÄúRtlfixer: Automatically fixing rtl syntax errors with large language model,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 61st ACM/IEEE Design Automation Conference</em>, ser. DAC ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3649329.3657353</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A.¬†Sarihi, A.¬†Patooghy, P.¬†Jamieson, and A.-H.¬†A. Badawy, ‚ÄúHardware trojan insertion using reinforcement learning,‚Äù 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J.¬†Cruz, P.¬†Gaikwad, A.¬†Nair, P.¬†Chakraborty, and S.¬†Bhunia, ‚ÄúAutomatic hardware trojan insertion using machine learning,‚Äù 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S.¬†R. Zantout, <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Hardware Trojan Detection in FPGA through Side-Channel Power Analysis and Machine Learning</em>.¬†¬†¬†University of California, Irvine, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M.¬†Mushtaq, M.¬†A. Mukhtar, V.¬†Lapotre, M.¬†K. Bhatti, and G.¬†Gogniat, ‚ÄúWinter is here! a decade of cache-based side-channel attacks, detection &amp; mitigation for rsa,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Information Systems</em>, vol.¬†92, p. 101524, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0306437920300338</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
H.¬†Wang, H.¬†Sayadi, A.¬†Sasan, S.¬†Rafatirad, T.¬†Mohsenin, and H.¬†Homayoun, ‚ÄúComprehensive evaluation of machine learning countermeasures for detecting microarchitectural side-channel attacks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 2020 on Great Lakes Symposium on VLSI</em>, ser. GLSVLSI ‚Äô20.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2020, p. 181‚Äì186. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3386263.3407586</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T.¬†Lavanya and K.¬†Rajalakshmi, ‚ÄúA review paper on machine learning based trojan detection in the iot chips,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Internet of Things and Connected Technologies</em>, R.¬†Misra, N.¬†Kesswani, M.¬†Rajarajan, B.¬†Veeravalli, and A.¬†Patel, Eds.¬†¬†¬†Cham: Springer International Publishing, 2022, pp. 225‚Äì238.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
K.¬†Liakos, G.¬†Georgakilas, and F.¬†Plessas, <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Hardware and System Security: Attacks and Countermeasures Against Hardware Trojans</em>.¬†¬†¬†Cham: Springer International Publishing, 2023, pp. 501‚Äì549. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1007/978-3-031-16344-9_13</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Z.¬†Pan and P.¬†Mishra, ‚ÄúA survey on hardware vulnerability analysis using machine learning,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">IEEE Access</em>, vol.¬†10, pp. 49‚Äâ508‚Äì49‚Äâ527, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
R.¬†Mukherjee, S.¬†R. Rajendran, and R.¬†S. Chakraborty, ‚ÄúA comprehensive survey of physical and logic testing techniques for hardware trojan detection and prevention,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Journal of Cryptographic Engineering</em>, vol.¬†12, no.¬†4, pp. 495‚Äì522, Nov 2022. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1007/s13389-022-00295-w</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
K.¬†I. Gubbi, B.¬†Saber¬†Latibari, A.¬†Srikanth, T.¬†Sheaves, S.¬†A. Beheshti-Shirazi, S.¬†M. PD, S.¬†Rafatirad, A.¬†Sasan, H.¬†Homayoun, and S.¬†Salehi, ‚ÄúHardware trojan detection using machine learning: A tutorial,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ACM Trans. Embed. Comput. Syst.</em>, vol.¬†22, no.¬†3, apr 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3579823</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
V.¬†Gohil, H.¬†Guo, S.¬†Patnaik, and J.¬†Rajendran, ‚ÄúAttrition: Attacking static hardware trojan detection techniques using reinforcement learning,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security</em>, ser. CCS ‚Äô22.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2022, p. 1275‚Äì1289. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3548606.3560690</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
K.¬†G. Liakos, G.¬†K. Georgakilas, S.¬†Moustakidis, N.¬†Sklavos, and F.¬†C. Plessas, ‚ÄúConventional and machine learning approaches as countermeasures against hardware trojan attacks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Microprocessors and Microsystems</em>, vol.¬†79, p. 103295, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0141933120304543</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
A.¬†Tauhid, L.¬†Xu, M.¬†Rahman, and E.¬†Tomai, ‚ÄúA survey on security analysis of machine learning-oriented hardware and software intellectual property,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">High-Confidence Computing</em>, vol.¬†3, no.¬†2, p. 100114, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S2667295223000120</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S.¬†Thakur, B.¬†Ahmad, Z.¬†Fan, H.¬†Pearce, B.¬†Tan, R.¬†Karri, B.¬†Dolan-Gavitt, and S.¬†Garg, ‚ÄúBenchmarking large language models for automated verilog rtl code generation,‚Äù 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
K.¬†Thorat, J.¬†Zhao, Y.¬†Liu, H.¬†Peng, X.¬†Xie, B.¬†Lei, J.¬†Zhang, and C.¬†Ding, ‚ÄúAdvanced language model-driven verilog development: Enhancing power, performance, and area optimization in code synthesis,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2312.01022</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
M.¬†E. Yanik, I.¬†Cicek, and E.¬†Afacan, ‚ÄúShortcircuit: An open-source chatgpt driven digital integrated circuit front-end design automation tool,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">2023 30th IEEE International Conference on Electronics, Circuits and Systems (ICECS)</em>, 2023, pp. 1‚Äì4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
S.¬†Thakur, B.¬†Ahmad, H.¬†Pearce, B.¬†Tan, B.¬†Dolan-Gavitt, R.¬†Karri, and S.¬†Garg, ‚ÄúVerigen: A large language model for verilog code generation,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M.¬†Li, W.¬†Fang, Q.¬†Zhang, and Z.¬†Xie, ‚ÄúSpecllm: Exploring generation and review of vlsi design specification with large language model,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2401.13266</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
B.¬†Mali, K.¬†Maddala, S.¬†Reddy, V.¬†Gupta, C.¬†Karfa, and R.¬†Karri, ‚ÄúChiraag: Chatgpt informed rapid and automated assertion generation,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
M.¬†DeLorenzo, A.¬†B. Chowdhury, V.¬†Gohil, S.¬†Thakur, R.¬†Karri, S.¬†Garg, and J.¬†Rajendran, ‚ÄúMake every move count: Llm-based high-quality rtl code generation using mcts,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
M.¬†Hassan, S.¬†Ahmadi-Pour, K.¬†Qayyum, C.¬†K. Jha, and R.¬†Drechsler, ‚ÄúLlm-guided formal verification coupled with mutation testing,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">2024 Design, Automation and Test in Europe Conference and Exhibition (DATE)</em>, 2024, pp. 1‚Äì2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
L.¬†J. Wan, Y.¬†Huang, Y.¬†Li, H.¬†Ye, J.¬†Wang, X.¬†Zhang, and D.¬†Chen, ‚ÄúInvited paper: Software/hardware co-design for llm and its application for design verification,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)</em>, 2024, pp. 435‚Äì441.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
X.¬†Yao, ‚ÄúHdldebugger: Streamlining hdl debugging with large language models,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2403.11671</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
K.¬†Chang, ‚ÄúData is all you need: Finetuning llms for chip design via an automated design-data augmentation framework,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2403.11202</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Z.¬†Wang, Y.¬†Shen, X.¬†Yao, W.¬†Zhao, Y.¬†Bai, F.¬†Farnia, and B.¬†Yu, ‚ÄúChatpattern: Layout pattern customization via natural language,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 61st ACM/IEEE Design Automation Conference</em>, ser. DAC ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3649329.3657361</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Anonymous, ‚ÄúImproving large language model hardware generating quality through post-LLM search,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Machine Learning for Systems 2023</em>, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=IY7M6sqCxq</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M.¬†DeLorenzo, ‚ÄúCreativeval: Evaluating creativity of llm-based hardware code generation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2404.08806</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
M.¬†Nazzal, ‚ÄúA dataset for large language model-driven ai accelerator generation,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
P.¬†Vitolo, ‚ÄúNatural language to verilog: Design of a recurrent spiking neural network using large language models and chatgpt,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2405.01419</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
M.¬†Xiang, ‚ÄúDigital asic design with ongoing llms: Strategies and prospects,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2405.02329</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J.¬†Blocklove, S.¬†Garg, R.¬†Karri, and H.¬†Pearce, ‚ÄúEvaluating llms for hardware design and test,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">2024 IEEE LLM Aided Design Workshop (LAD)</em>, 2024, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
S.¬†Liu, ‚ÄúRtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
B.¬†Nadimi, ‚ÄúA multi-expert large language model architecture for verilog code generation,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
B.¬†Ahmad, S.¬†Thakur, B.¬†Tan, R.¬†Karri, and H.¬†Pearce, ‚ÄúOn hardware security bug code fixes by prompting large language models,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">IEEE Transactions on Information Forensics and Security</em>, vol.¬†19, pp. 4043‚Äì4057, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
W.¬†Fu, S.¬†Li, Y.¬†Zhao, H.¬†Ma, R.¬†Dutta, X.¬†Zhang, K.¬†Yang, Y.¬†Jin, and X.¬†Guo, ‚ÄúHardware phi-1.5b: A large language model encodes hardware domain specific knowledge,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 29th Asia and South Pacific Design Automation Conference</em>, ser. ASPDAC ‚Äô24.¬†¬†¬†IEEE Press, 2024, p. 349‚Äì354. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1109/ASP-DAC58780.2024.10473927</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J.¬†T. Meech, ‚ÄúLeveraging high-level synthesis and large language models to generate, simulate, and deploy a uniform random number generator hardware design,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2311.03489</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
J.¬†Liu, A.¬†Sharma, C.¬†Doumbia, and J.¬†K.¬†S. Poon, ‚ÄúTowards large-language model assisted layout of silicon photonic integrated circuits,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">The 25th European Conference on Integrated Optics</em>, J.¬†Witzens, J.¬†Poon, L.¬†Zimmermann, and W.¬†Freude, Eds.¬†¬†¬†Cham: Springer Nature Switzerland, 2024, pp. 441‚Äì447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
K.¬†Chang, Z.¬†Chen, Y.¬†Zhou, W.¬†Zhu, kun wang, H.¬†Xu, C.¬†Li, M.¬†Wang, S.¬†Liang, H.¬†Li, Y.¬†Han, and Y.¬†Wang, ‚ÄúNatural language is not enough: Benchmarking multi-modal generative ai for verilog generation,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.08473</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Y.¬†Zhao, D.¬†Huang, C.¬†Li, P.¬†Jin, Z.¬†Nan, T.¬†Ma, L.¬†Qi, Y.¬†Pan, Z.¬†Zhang, R.¬†Zhang, X.¬†Zhang, Z.¬†Du, Q.¬†Guo, X.¬†Hu, and Y.¬†Chen, ‚ÄúCodev: Empowering llms for verilog generation through multi-level summarization,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.10424</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Z.¬†Jiang, Q.¬†Zhang, C.¬†Liu, H.¬†Li, and X.¬†Li, ‚ÄúIicpilot: An intelligent integrated circuit backend design framework using open eda,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.12576</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
M.¬†Abdelatty and S.¬†Reda, ‚ÄúHdlcopilot: Hardware design library querying with natural language,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.12749</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Z.¬†He, H.¬†Wu, X.¬†Zhang, X.¬†Yao, S.¬†Zheng, H.¬†Zheng, and B.¬†Yu, ‚ÄúChateda: A large language model powered autonomous agent for eda,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2308.10204</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
C.-T. Ho and H.¬†Ren, ‚ÄúLarge language model (llm) for standard cell layout design optimization,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.06549</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
M.¬†Gao, J.¬†Zhao, Z.¬†Lin, W.¬†Ding, X.¬†Hou, Y.¬†Feng, C.¬†Li, and M.¬†Guo, ‚ÄúAutovcoder: A systematic framework for automated verilog code generation using llms,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.18333</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
A.¬†Nakkab, S.¬†Q. Zhang, R.¬†Karri, and S.¬†Garg, ‚ÄúRome was not built in a single step: Hierarchical prompting for llm-based chip design,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.18276</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
L.¬†Collini, S.¬†Garg, and R.¬†Karri, ‚ÄúC2hlsc: Can llms bridge the software-to-hardware design gap?‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2406.09233</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
A.¬†B. Chowdhury, M.¬†Romanelli, B.¬†Tan, R.¬†Karri, and S.¬†Garg, ‚ÄúRetrieval-guided reinforcement learning for boolean circuit minimization,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">The Twelfth International Conference on Learning Representations</em>, 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=0t1O8ziRZp</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
R.¬†Qiu, G.¬†L. Zhang, R.¬†Drechsler, U.¬†Schlichtmann, and B.¬†Li, ‚ÄúAutobench: Automatic testbench generation and evaluation using llms for hdl design,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD</em>.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3670474.3685956</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
S.¬†Qiu, B.¬†Tan, and H.¬†Pearce, ‚ÄúLlm-aided explanations of eda synthesis errors,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">2024 IEEE LLM Aided Design Workshop (LAD)</em>, 2024, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Y.¬†Lai, S.¬†Lee, G.¬†Chen, S.¬†Poddar, M.¬†Hu, D.¬†Z. Pan, and P.¬†Luo, ‚ÄúAnalogcoder: Analog circuit design via training-free code generation,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2405.14918</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
C.-C. Chang, Y.¬†Shan, S.¬†Fan, J.¬†Li, S.¬†Zhang, N.¬†Cao, Y.¬†Chen, and X.¬†Zhang, ‚ÄúLamagic: Language-model-based topology generation for analog integrated circuits,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.18269</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
G.¬†Chen, K.¬†Zhu, S.¬†Kim, H.¬†Zhu, Y.¬†Lai, B.¬†Yu, and D.¬†Z. Pan, ‚ÄúLlm-enhanced bayesian optimization for efficient analog layout constraint generation,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.05250</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Y.¬†Yin, Y.¬†Wang, B.¬†Xu, and P.¬†Li, ‚ÄúAdo-llm: Analog design bayesian optimization with in-context learning of large language models,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.18770</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
B.¬†Liu, H.¬†Zhang, X.¬†Gao, Z.¬†Kong, X.¬†Tang, Y.¬†Lin, R.¬†Wang, and R.¬†Huang, ‚ÄúLayoutcopilot: An llm-powered multi-agent collaborative framework for interactive analog layout design,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.18873</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Y.¬†Liu, Z.¬†Ju, Z.¬†Li, M.¬†Dong, H.¬†Zhou, J.¬†Wang, F.¬†Yang, X.¬†Zeng, and L.¬†Shang, ‚ÄúFloorplanning with graph attention,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the 59th ACM/IEEE Design Automation Conference</em>, ser. DAC ‚Äô22.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2022, p. 1303‚Äì1308. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3489517.3530484</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
T.¬†Mohamed, V.¬†M. van Santen, L.¬†Alrahis, O.¬†Sinanoglu, and H.¬†Amrouch, ‚ÄúGraph attention networks to identify the impact of transistor degradation on circuit reliability,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">IEEE Transactions on Circuits and Systems I: Regular Papers</em>, vol.¬†71, no.¬†7, pp. 3269‚Äì3281, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
V.¬†Lee, C.¬†Deng, L.¬†Elzeiny, P.¬†Abbeel, and J.¬†Wawrzynek, ‚ÄúChip placement with diffusion,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.12282</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
X.¬†Li, X.¬†Li, L.¬†Chen, X.¬†Zhang, M.¬†Yuan, and J.¬†Wang, ‚ÄúCircuit transformer: End-to-end circuit design by predicting the next gate,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2403.13838</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
C.-T. Ho, A.¬†Chandna, D.¬†Guan, A.¬†Ho, M.¬†Kim, Y.¬†Li, and H.¬†Ren, ‚ÄúNovel transformer model based clustering method for standard cell design automation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Proceedings of the 2024 International Symposium on Physical Design</em>, ser. ISPD ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024, p. 195‚Äì203. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3626184.3633314</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
C.¬†Deng, Z.¬†Yue, C.¬†Yu, G.¬†Sarar, R.¬†Carey, R.¬†Jain, and Z.¬†Zhang, ‚ÄúLess is more: Hop-wise graph attention for scalable and generalizable learning on circuits,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2403.01317</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
B.¬†Zhu, R.¬†Chen, X.¬†Zhang, F.¬†Yang, X.¬†Zeng, B.¬†Yu, and M.¬†D. Wong, ‚ÄúHotspot detection via multi-task learning and transformer encoder,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)</em>, 2021, pp. 1‚Äì8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Y.¬†Lai, J.¬†Liu, Z.¬†Tang, B.¬†Wang, J.¬†Hao, and P.¬†Luo, ‚ÄúChipformer: Transferable chip placement via offline decision transformer,‚Äù 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2306.14744</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
A.¬†Mehradfar, X.¬†Zhao, Y.¬†Niu, S.¬†Babakniya, M.¬†Alesheikh, H.¬†Aghasi, and S.¬†Avestimehr, ‚ÄúAicircuit: A multi-level dataset and benchmark for ai-driven analog integrated circuit design,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.18272</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Y.-Z. Lin, M.¬†Mamun, M.¬†A. Chowdhury, S.¬†Cai, M.¬†Zhu, B.¬†S. Latibari, K.¬†I. Gubbi, N.¬†N. Bavarsad, A.¬†Caputo, A.¬†Sasan <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">et¬†al.</em>, ‚ÄúHw-v2w-map: Hardware vulnerability to weakness mapping framework for root cause analysis with gpt-assisted mitigation suggestion,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
D.¬†Saha, K.¬†Yahyaei, S.¬†Kumar¬†Saha, M.¬†Tehranipoor, and F.¬†Farahmandi, ‚ÄúEmpowering hardware security with llm: The development of a vulnerable hardware database,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">2024 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)</em>, 2024, pp. 233‚Äì243.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
M.¬†Akyash, ‚ÄúSelf-hwdebug: Automation of llm self-instructing for hardware security verification,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
D.¬†Saha, ‚ÄúLlm for soc security: A paradigm shift,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
D.¬†N. Gadde, A.¬†Kumar, T.¬†Nalapat, E.¬†Rezunov, and F.¬†Cappellini, ‚ÄúAll artificial, less intelligence: Genai through the lens of formal verification,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2403.16750</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
R.¬†Kande, H.¬†Pearce, B.¬†Tan, B.¬†Dolan-Gavitt, S.¬†Thakur, R.¬†Karri, and J.¬†Rajendran, ‚ÄúLlm-assisted generation of hardware assertions,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
J.¬†Chaudhuri, D.¬†Thapar, A.¬†Chaudhuri, F.¬†Firouzi, and K.¬†Chakrabarty, ‚ÄúSpiced+: Syntactical bug pattern identification and correction of trojans in a/ms circuits using llm-enhanced detection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</em>, vol.¬†33, no.¬†4, pp. 1118‚Äì1131, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
X.¬†Meng, ‚ÄúUnlocking hardware security assurance: The potential of llms,‚Äù 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
S.¬†Tarek, D.¬†Saha, S.¬†K. Saha, M.¬†Tehranipoor, and F.¬†Farahmandi, ‚ÄúSocurellm: An llm-driven approach for large-scale system-on-chip security verification and policy generation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Cryptology ePrint Archive</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
G.¬†Kokolakis, A.¬†Moschos, and A.¬†D. Keromytis, ‚ÄúHarnessing the power of general-purpose llms in hardware trojan design,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">International Conference on Applied Cryptography and Network Security</em>.¬†¬†¬†Springer, 2024, pp. 176‚Äì194.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
V.¬†T. Hayashi and W.¬†V. Ruggiero, ‚ÄúHardware trojan dataset of risc-v and web3 generated with chatgpt-4,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Data</em>, vol.¬†9, no.¬†6, p.¬†82, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
J.¬†Bhandari, R.¬†Sadhukhan, P.¬†Krishnamurthy, F.¬†Khorrami, and R.¬†Karri, ‚ÄúSentaur: Security enhanced trojan assessment using llms against undesirable revisions,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.12352</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
J.¬†Chaudhuri, A.¬†Chaudhuri, and K.¬†Chakrabarty, ‚ÄúLatent: Llm-augmented trojan insertion and evaluation framework for analog netlist topologies,‚Äù 2025. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2505.06364</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
B.¬†S. Latibari, S.¬†Ghimire, M.¬†A. Chowdhury, N.¬†Nazari, K.¬†I. Gubbi, H.¬†Homayoun, A.¬†Sasan, and S.¬†Salehi, ‚ÄúAutomated hardware logic obfuscation framework using gpt,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">2024 IEEE 17th Dallas Circuits and Systems Conference (DCAS)</em>, 2024, pp. 1‚Äì5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
A.¬†Srivastava, S.¬†Das, N.¬†Choudhury, R.¬†Psiakis, P.¬†H. Silva, D.¬†Pal, and K.¬†Basu, ‚ÄúScar: Power side-channel analysis at rtl level,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</em>, vol.¬†32, no.¬†6, pp. 1110‚Äì1123, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Z.¬†He and R.¬†B. Lee, ‚ÄúHow secure is your cache against side-channel attacks?‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture</em>, 2017, pp. 341‚Äì353.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
M.¬†Chen, X.¬†Kou, and G.¬†Zhang, ‚ÄúTrojanformer: Resource-efficient hardware trojan detection using graph transformer network,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">2024 7th International Conference on Electronics Technology (ICET)</em>, 2024, pp. 165‚Äì170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Y.¬†Li, S.¬†Li, and H.¬†Shen, ‚ÄúHtrans: Transformer-based method for hardware trojan detection and localization,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">2023 IEEE 32nd Asian Test Symposium (ATS)</em>, 2023, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
H.¬†Zhang, Z.¬†Fan, Y.¬†Zhou, and Y.¬†Li, ‚ÄúB-htrecognizer: Bit-wise hardware trojan localization using graph attention networks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, pp. 1‚Äì1, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
R.¬†Afsharmazayejani, M.¬†M. Shahmiri, P.¬†Link, H.¬†Pearce, and B.¬†Tan, ‚ÄúToward hardware security benchmarking of llms,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">2024 IEEE LLM Aided Design Workshop (LAD)</em>, 2024, pp. 1‚Äì7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
M.¬†U. Hadi, al¬†tashi, R.¬†Qureshi, A.¬†Shah, M.¬†Irfan, A.¬†Zafar, M.¬†B. Shaikh, N.¬†Akhtar, J.¬†Wu, S.¬†Mirjalili, Q.¬†Al-Tashi, A.¬†Muneer, M.¬†A. Al-garadi, G.¬†Cnn, and T.¬†RoBERTa, ‚ÄúLarge language models: A comprehensive survey of its applications, challenges, limitations, and future prospects.‚Äù [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://api.semanticscholar.org/CorpusID:266378240</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Z.¬†Wang, L.¬†Alrahis, L.¬†Mankali, J.¬†Knechtel, and O.¬†Sinanoglu, ‚ÄúLlms and the future of chip design: Unveiling security risks and building trust,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2405.07061</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
M.¬†Liu, T.-D. Ene, R.¬†Kirby, C.¬†Cheng, N.¬†Pinckney, R.¬†Liang, J.¬†Alben, H.¬†Anand, S.¬†Banerjee, I.¬†Bayraktaroglu, B.¬†Bhaskaran, B.¬†Catanzaro, A.¬†Chaudhuri, S.¬†Clay, B.¬†Dally, L.¬†Dang, P.¬†Deshpande, S.¬†Dhodhi, S.¬†Halepete, E.¬†Hill, J.¬†Hu, S.¬†Jain, A.¬†Jindal, B.¬†Khailany, G.¬†Kokai, K.¬†Kunal, X.¬†Li, C.¬†Lind, H.¬†Liu, S.¬†Oberman, S.¬†Omar, G.¬†Pasandi, S.¬†Pratty, J.¬†Raiman, A.¬†Sarkar, Z.¬†Shao, H.¬†Sun, P.¬†P. Suthar, V.¬†Tej, W.¬†Turner, K.¬†Xu, and H.¬†Ren, ‚ÄúChipnemo: Domain-adapted llms for chip design,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
R.¬†Kande, V.¬†Gohil, M.¬†DeLorenzo, C.¬†Chen, and J.¬†Rajendran, ‚ÄúLlms for hardware security: Boon or bane?‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">2024 IEEE 42nd VLSI Test Symposium (VTS)</em>.¬†¬†¬†Los Alamitos, CA, USA: IEEE Computer Society, apr 2024, pp. 1‚Äì4. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.ieeecomputersociety.org/10.1109/VTS60656.2024.10538871</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
T.¬†B. Brown, B.¬†Mann, N.¬†Ryder, M.¬†Subbiah, J.¬†Kaplan, P.¬†Dhariwal, A.¬†Neelakantan, P.¬†Shyam, G.¬†Sastry, A.¬†Askell, S.¬†Agarwal, A.¬†Herbert-Voss, G.¬†Krueger, T.¬†Henighan, R.¬†Child, A.¬†Ramesh, D.¬†M. Ziegler, J.¬†Wu, C.¬†Winter, C.¬†Hesse, M.¬†Chen, E.¬†Sigler, M.¬†Litwin, S.¬†Gray, B.¬†Chess, J.¬†Clark, C.¬†Berner, S.¬†McCandlish, A.¬†Radford, I.¬†Sutskever, and D.¬†Amodei, ‚ÄúLanguage models are few-shot learners,‚Äù 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Z.¬†Wei, ‚ÄúTowards transferable adversarial attacks on vision transformers,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.¬†36, no.¬†3, 2022, pp. 2668‚Äì2676.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Y.-C. Lin, ‚ÄúNovel preprocessing technique for data embedding in engineering code generation using large language model,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
B.-Y. Wu, ‚ÄúEda corpus: A large language model dataset for enhanced interaction with openroad,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Y.¬†Lai, S.¬†Lee, G.¬†Chen, S.¬†Poddar, M.¬†Hu, D.¬†Z. Pan, and P.¬†Luo, ‚ÄúAnalogcoder: Analog circuit design via training-free code generation,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
S.¬†A. Sheikholeslam and A.¬†Ivanov, ‚ÄúSynthai: A multi agent generative ai framework for automated modular hls design generation,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
M.¬†A. Khair, J.¬†R. P.¬†K. Ande, D.¬†R. Goda, and S.¬†R. Yerram, ‚ÄúSecure vlsi design: Countermeasures against hardware trojans and side-channel attacks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">Engineering International</em>, vol.¬†7, p. 147‚Äì160, 12 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
H.¬†Huang, Z.¬†Lin, Z.¬†Wang, X.¬†Chen, K.¬†Ding, and J.¬†Zhao, ‚ÄúTowards llm-powered verilog rtl assistant: Self-verification and self-correction,‚Äù 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
A.¬†Alaql, S.¬†Chattopadhyay, P.¬†Chakraborty, T.¬†Hoque, and S.¬†Bhunia, ‚ÄúLego: A learning-guided obfuscation framework for hardware ip protection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, vol.¬†41, no.¬†4, pp. 854‚Äì867, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
A.¬†Nahiyan, K.¬†Xiao, K.¬†Yang, Y.¬†Jin, D.¬†Forte, and M.¬†Tehranipoor, ‚ÄúAvfsm: A framework for identifying and mitigating vulnerabilities in fsms,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">Proceedings of the 53rd Annual Design Automation Conference</em>, 2016, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
R.¬†Hassan, G.¬†Kolhe, S.¬†Rafatirad, H.¬†Homayoun, and S.¬†M.¬†P. Dinakarrao, ‚ÄúA neural network-based cognitive obfuscation toward enhanced logic locking,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, vol.¬†41, no.¬†11, pp. 4587‚Äì4599, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
S.¬†Paria, A.¬†Dasgupta, and S.¬†Bhunia, ‚ÄúNavigating soc security landscape on llm-guided paths,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">Proceedings of the Great Lakes Symposium on VLSI 2024</em>, ser. GLSVLSI ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024, p. 252‚Äì257. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3649476.3660393</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
M.¬†Shao, S.¬†Jancheska, M.¬†Udeshi, B.¬†Dolan-Gavitt, H.¬†Xi, K.¬†Milner, B.¬†Chen, M.¬†Yin, S.¬†Garg, P.¬†Krishnamurthy, F.¬†Khorrami, R.¬†Karri, and M.¬†Shafique, ‚ÄúNyu ctf dataset: A scalable open-source benchmark dataset for evaluating llms in offensive security,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.05590</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
S.¬†S. Miftah, A.¬†Srivastava, H.¬†Kim, and K.¬†Basu, ‚ÄúAssert-o: Context-based assertion optimization using llms,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Proceedings of the Great Lakes Symposium on VLSI 2024</em>, ser. GLSVLSI ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024, p. 233‚Äì239. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3649476.3660378</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
F.¬†Cui, C.¬†Yin, K.¬†Zhou, Y.¬†Xiao, G.¬†Sun, Q.¬†Xu, Q.¬†Guo, D.¬†Song, D.¬†Lin, X.¬†Zhang, Yun, and Liang, ‚ÄúOrigen:enhancing rtl code generation with code-to-code augmentation and self-reflection,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.16237</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Y.¬†Zhang, Z.¬†Yu, Y.¬†Fu, C.¬†Wan, and Y.¬†C. Lin, ‚ÄúMg-verilog: Multi-grained dataset towards enhanced llm-assisted verilog generation,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.01910</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Z.¬†He and B.¬†Yu, ‚ÄúLarge language models for eda: Future or mirage?‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">Proceedings of the 2024 International Symposium on Physical Design</em>, ser. ISPD ‚Äô24.¬†¬†¬†New York, NY, USA: Association for Computing Machinery, 2024, p. 65‚Äì66. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3626184.3639700</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
N.¬†Rouf, F.¬†Amin, and P.¬†D. Franzon, ‚ÄúCan low-rank knowledge distillation in llms be useful for microelectronic reasoning?‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.13808</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
X.¬†Li, X.¬†Li, L.¬†Chen, X.¬†Zhang, M.¬†Yuan, and J.¬†Wang, ‚ÄúLogic synthesis with generative deep neural networks,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2406.04699</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
A.¬†B. Chowdhury, M.¬†Romanelli, B.¬†Tan, R.¬†Karri, and S.¬†Garg, ‚ÄúRetrieval-guided reinforcement learning for boolean circuit minimization,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2401.12205</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
D.¬†Saha, S.¬†Tarek, K.¬†Yahyaei, S.¬†K. Saha, J.¬†Zhou, M.¬†Tehranipoor, and F.¬†Farahmandi, ‚ÄúLlm for soc security: A paradigm shift,‚Äù 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2310.06046</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Y.¬†Pu, Z.¬†He, T.¬†Qiu, H.¬†Wu, and B.¬†Yu, ‚ÄúCustomized retrieval augmented generation and benchmarking for eda tool documentation qa,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.15353</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
N.¬†Pinckney, C.¬†Batten, M.¬†Liu, H.¬†Ren, and B.¬†Khailany, ‚ÄúRevisiting verilogeval: Newer llms, in-context learning, and specification-to-rtl tasks,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2408.11053</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
E.¬†Nijkamp, B.¬†Pang, H.¬†Hayashi, L.¬†Tu, H.¬†Wang, Y.¬†Zhou, S.¬†Savarese, and C.¬†Xiong, ‚ÄúCodegen: An open large language model for code with multi-turn program synthesis,‚Äù 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2203.13474</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
J.¬†Zhou, G.¬†Cui, S.¬†Hu, Z.¬†Zhang, C.¬†Yang, Z.¬†Liu, L.¬†Wang, C.¬†Li, and M.¬†Sun, ‚ÄúGraph neural networks: A review of methods and applications,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">AI Open</em>, vol.¬†1, pp. 57‚Äì81, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S2666651021000012</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
N.¬†Nazari, H.¬†M. Makrani, C.¬†Fang, H.¬†Sayadi, S.¬†Rafatirad, K.¬†N. Khasawneh, and H.¬†Homayoun, ‚ÄúForget and rewire: Enhancing the resilience of transformer-based models against Bit-Flip attacks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">33rd USENIX Security Symposium (USENIX Security 24)</em>.¬†¬†¬†Philadelphia, PA: USENIX Association, Aug. 2024, pp. 1349‚Äì1366. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.usenix.org/conference/usenixsecurity24/presentation/nazari</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 16 22:22:58 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
